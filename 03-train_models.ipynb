{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41d4cb72-a8fc-4f07-9597-01f4d14d0773"
   },
   "source": [
    "## Training Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7de6dcd7-ed68-41c8-9f52-e831fa70b490"
   },
   "source": [
    "### The following cell is a way to get the utility script required for this notebook. \n",
    "Since IBM CPD SaaS doesn't have a filesystem, this is the only reliable way to get scripts on the cloud environment.\n",
    "\n",
    "```\n",
    "!rm -rf MLOps-CPD && git clone --quiet -b master https://github.com/IBM/MLOps-CPD.git\n",
    "```\n",
    "\n",
    "⚠️ Run the following cells only if you are executing on IBM CPD SaaS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62f40125-2e48-42cc-bee1-d58c179f372d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!rm -rf MLOps-CPD && git clone --quiet -b master https://github.com/IBM/MLOps-CPD.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddc8c99d-9b96-4288-bf26-626671173f5d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!mv MLOps-CPD MLOps_CPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a3f81ef-bef9-47a0-8d4c-f62ed9201a47",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebc3adc3-6640-453e-8cfa-19ccdb28107b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install hyperopt ibm_aigov_facts_client ibm_watson_machine_learning ibm_watson_studio_pipelines lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31eeb3f9-7425-4f01-8685-b2d3377ca15a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botocore.client import Config\n",
    "from ibm_botocore.client import Config\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix,plot_confusion_matrix,plot_roc_curve,f1_score,auc,roc_curve,accuracy_score\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from ibm_aigov_facts_client import AIGovFactsClient, CloudPakforDataConfig\n",
    "from ibm_watson_studio_pipelines import WSPipelines\n",
    "from ibm_watson_machine_learning import APIClient\n",
    "import warnings\n",
    "import os, types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ibm_boto3\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import exists\n",
    "if exists(\"utils/fs_utils.py\") and exists(\"utils/catalog_utils.py\"):\n",
    "    from utils import fs_utils,catalog_utils\n",
    "else:\n",
    "    # If utils/fs_utils.py and utils/catalog_utils.py exist we assume that you are running on CPD SaaS\n",
    "    # and will therefore import scripts from the freshly cloned repository\n",
    "    from MLOps_CPD.utils import fs_utils, catalog_utils\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import seaborn  as sns\n",
    "except Exception as e:\n",
    "    print(str(e) + \" - Optional pluging skipped\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7732991-ef1e-4050-9be1-2d6d6e0cd366"
   },
   "source": [
    "## Pipeline Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d0e423b-2524-4f4a-8e2c-e277807159e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## REMOTE AUTH\n",
    "\n",
    "TOKEN = os.getenv(\"USER_ACCESS_TOKEN\")\n",
    "\n",
    "CPD_HOST = os.getenv(\"CPD_HOST\")\n",
    "CPD_USERNAME = os.getenv(\"CPD_USERNAME\")\n",
    "CPD_PASSWORD = os.getenv(\"CPD_PASSWORD\")\n",
    "\n",
    "## LOCAL FS\n",
    "\n",
    "path = os.getenv(\"path\")\n",
    "\n",
    "pipeline_filename = os.getenv(\"pipeline_filename\")\n",
    "\n",
    "train_data_filename = os.getenv(\"train_data_filename\")\n",
    "test_data_filename = os.getenv(\"test_data_filename\")\n",
    "\n",
    "### Construct paths\n",
    "train_data_path = os.path.join(path, train_data_filename)\n",
    "test_data_path = os.path.join(path, test_data_filename)\n",
    "\n",
    "pipeline_path = os.path.join(path, pipeline_filename)\n",
    "\n",
    "## WML / Factsheets\n",
    "\n",
    "EXPERIMENT_NAME = os.getenv(\"EXPERIMENT_NAME\")\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "DEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "\n",
    "SPACE_ID = os.getenv(\"SPACE_ID\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85d8a7b1-e919-449f-ac56-59b363b78992"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe4004ab-fc88-4d5c-8e7c-aa74001c4ccd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_for_file_in_filesystem(path):\n",
    "    \"\"\"\n",
    "    Check existence of path in filesystem\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        return True\n",
    "    else:\n",
    "        print(\"File not found in specified path.\")\n",
    "        return False     \n",
    "\n",
    "def load_model(key, filename):\n",
    "    check_for_file_in_filesystem(filename)\n",
    "    with open (filename,\"rb\") as f:\n",
    "        pipeline = pickle.load(f)\n",
    "    return pipeline\n",
    "\n",
    "def load_data_from_filesystem(path):\n",
    "    \"\"\"\n",
    "    Check existence of path in filesystem.\n",
    "    If it does exist, loads csv via path\n",
    "    If it does NOT exist, try to load data from Db2\n",
    "    \"\"\"\n",
    "    body = check_for_file_in_filesystem(path)\n",
    "    if body:\n",
    "        suffix = path[-3:]\n",
    "        # Check whether path ends on csv\n",
    "        if suffix == \"csv\":\n",
    "            gcf_df = pd.read_csv(path)\n",
    "        else:\n",
    "            with open(path) as f:\n",
    "                gcf_df = pickle.load(f)\n",
    "\n",
    "        return gcf_df\n",
    "    else:\n",
    "        print(\"\\n\")\n",
    "        print(f\"{path} file/path is probably not in project. Loading File from MLOps COS Bucket.\")\n",
    "\n",
    "        data_request = {\n",
    "                'connection_name': \"\"\"DB2_DATA\"\"\",\n",
    "                'interaction_properties': {\n",
    "                    'select_statement': 'SELECT * FROM \"CUSTOMER_DATA\".\"GERMAN_CREDIT_RISK_TRAINING\" FETCH FIRST 5000 ROWS ONLY'\n",
    "                }\n",
    "            }\n",
    "\n",
    "        gcf_df = read_data_from_db2(data_request)\n",
    "        return gcf_df\n",
    "\n",
    "\n",
    "def read_data_from_db2(data_request):\n",
    "    \"\"\"\n",
    "    \n",
    "    If load_data_from_filesystem fails, this method is executed.\n",
    "    \"\"\"\n",
    "    read_client = itcfs.get_flight_client()\n",
    "    DB2_DATA_data_request = {\n",
    "        'connection_name': \"\"\"DB2_DATA\"\"\",\n",
    "        'interaction_properties': {\n",
    "            'select_statement': 'SELECT * FROM \"CUSTOMER_DATA\".\"GERMAN_CREDIT_RISK_TRAINING\" FETCH FIRST 5000 ROWS ONLY'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    flightInfo = itcfs.get_flight_info(read_client, nb_data_request=data_request)\n",
    "\n",
    "    df = itcfs.read_pandas_and_concat(read_client, flightInfo, timeout=240)\n",
    "    return df\n",
    "\n",
    "def save_data_in_filesystem(df,filename):\n",
    "    \"\"\"\n",
    "    Save Data in Filesystem\n",
    "\n",
    "    Passed filename should involve path\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if filename[-3:] == \"csv\":\n",
    "            df.to_csv(filename,index=False)\n",
    "            print(f\"File {filename} persisted successfully as csv\")\n",
    "        else:\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(df, f)\n",
    "            print(f\"File {filename} pickled successfully\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"File serialization for {filename} failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a436b9b0-57e2-4fa5-883d-665c0f9c564d"
   },
   "source": [
    "## Load the Saved Transformer from IBM COS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "015b3d05-7886-489b-845f-b56cbc207f54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = load_model(pipeline_path, pipeline_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eb8ddb8-9737-44b7-84bd-b3e37a90b25d"
   },
   "source": [
    "## Instantiate FactSheets Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff993d79-0543-42ad-bd91-1c8c395c7327",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set-up Factsheets\n",
    "cpd_creds = CloudPakforDataConfig(service_url=CPD_HOST, username=CPD_USERNAME, password=CPD_PASSWORD)\n",
    "    \n",
    "if SPACE_ID is None:\n",
    "    # Only of SPACE_ID is unavailble, fallback to PROJECT_ID\n",
    "    if PROJECT_ID is None:\n",
    "        # Neither SPACE_ID nor PROJECT_ID available\n",
    "        pass\n",
    "    else:\n",
    "        # PROJECT_ID available\n",
    "        deploy_to = 'project'\n",
    "        try:\n",
    "            # Create new experiment\n",
    "            facts_client = AIGovFactsClient(experiment_name=EXPERIMENT_NAME,container_type=deploy_to,container_id=PROJECT_ID,cloud_pak_for_data_configs=cpd_creds)\n",
    "        except Exception as e:\n",
    "            # Experiment already exists.. set as current experiment\n",
    "            facts_client = AIGovFactsClient(experiment_name=EXPERIMENT_NAME,container_type=deploy_to,container_id=PROJECT_ID,cloud_pak_for_data_configs=cpd_creds,set_as_current_experiment=True)\n",
    "        else:\n",
    "            print(e)\n",
    "else:\n",
    "    # SPACE_ID is available\n",
    "    deploy_to = 'space'\n",
    "    try:\n",
    "        # Create new experiment\n",
    "        facts_client = AIGovFactsClient(experiment_name=EXPERIMENT_NAME,container_type=deploy_to,container_id=SPACE_ID,cloud_pak_for_data_configs=cpd_creds)\n",
    "    except Exception as e:\n",
    "        # Experiment already exists.. set as current experiment\n",
    "        facts_client = AIGovFactsClient(experiment_name=EXPERIMENT_NAME,container_type=deploy_to,container_id=SPACE_ID,cloud_pak_for_data_configs=cpd_creds,set_as_current_experiment=True)\n",
    "    else:\n",
    "        print(e)\n",
    "        \n",
    "print(f\"Setting up Factsheets in {deploy_to}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f8d20e1-076d-4eea-a641-72fec780e02d"
   },
   "source": [
    "## Load Train Data and Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45e77429-df02-4ea8-bc5d-5dac9298bf0e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = load_data_from_filesystem(train_data_path)\n",
    "test_data = load_data_from_filesystem(test_data_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd21e793-0368-41de-9782-380a6cadff40"
   },
   "source": [
    "## Load train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e78e6e7-254d-48be-8617-62967ae6dc9f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = train_data['Risk']\n",
    "\n",
    "y_test = test_data['Risk']\n",
    "\n",
    "X_train = train_data.drop('Risk',axis=1)\n",
    "\n",
    "X_test = test_data.drop('Risk',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20bad83e-0df3-4491-ac22-615351ba7763"
   },
   "source": [
    "## Make validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "711605c8-8e95-47ef-b88e-8ce9f2a72012",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5491174d-106a-42d6-b652-b0421bf13739"
   },
   "source": [
    "## Instantiate a Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ba219af-4c27-4450-8424-e5a901b05ffd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model_lgb = LGBMClassifier(learning_rate=0.09,max_depth=5,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bbcbb48-ce9f-4633-aa56-f45baf5ddff7"
   },
   "source": [
    "## Append the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30ecedff-a09b-4c0f-9943-8eb3e02931fa",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator_step = ['model_lgb',model_lgb]\n",
    "pipeline.steps.append(estimator_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24e055c3-9a8c-4fb2-a5fc-bb818d15211a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.steps[0][1].fit(X_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8e83e93-668b-4dfc-91a0-201970176d21"
   },
   "source": [
    "## Baseline Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a2dec2a-24c0-4cde-9a73-e312fcced8ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_pipeline = pipeline.fit(X_train,y_train)\n",
    "model_pipeline = pipeline.fit(X_tr,y_tr,model_lgb__verbose=5, model_lgb__eval_set=[(pipeline.steps[0][1].transform(X_val), y_val),(pipeline.steps[0][1].transform(X_tr), y_tr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6a3a3bd-899f-404b-b520-cb74dac309b7",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightgbm import plot_metric\n",
    "plot_metric(pipeline.steps[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5374cd8d-5770-43cc-9e7e-43bb3056f58d"
   },
   "source": [
    "## Log the Train and Val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90c77145-f82d-4609-8ebf-b209f326b628",
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_loss = pipeline.steps[1][1].evals_result_['valid_0']\n",
    "train_loss = pipeline.steps[1][1].evals_result_['valid_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f0b33ff-fc57-4446-8ddb-c0aaeab364de"
   },
   "source": [
    "## Save train and val loss to COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8a6ea57-75fc-4f76-afe8-d65e6c0395da",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_loss_path = os.path.join(path, 'val_loss.pkl')\n",
    "save_data_in_filesystem(val_loss, val_loss_path)\n",
    "\n",
    "train_loss_path = os.path.join(path, 'train_loss.pkl')\n",
    "save_data_in_filesystem(train_loss, train_loss_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44bfa402-1452-4875-862d-73026bafdfe9"
   },
   "source": [
    "## Check if the files are copied in COS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a547554-19b3-4cb2-9190-2089760ca5e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_copied_in_cos = check_for_file_in_filesystem(val_loss_path) and check_for_file_in_filesystem(train_loss_path)\n",
    "files_copied_in_cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de2807dc-d3c4-493a-80a1-377ff7767aaf"
   },
   "source": [
    "## Baseline Results of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77f9f294-509d-4942-8e0b-0c7d480b913c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "print(roc_auc_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdf00db8-63d7-46e0-8cdc-821abf7893ab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "scores = np.mean(cross_val_score(pipeline,X_train,y_train, cv=cv, n_jobs=-1,scoring='roc_auc'))\n",
    "print(f\"The Cross Validated AUC_ROC Score is {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4697e42a-41be-440d-8059-5ca540bbce10",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(pipeline.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(pipeline.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a55898dd-d535-45b8-abe8-6a059c4cc65d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cm = confusion_matrix(y_test,predictions)\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in df_cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in df_cm.flatten()/np.sum(df_cm)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2,v3 in zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "try:\n",
    "    sns.heatmap(df_cm, annot=labels, fmt='', cmap='Blues')\n",
    "except Exception as e:\n",
    "    print(str(e) + \" - Seaborn missing, skipping optional heatmap plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d75dfe7-1921-4eca-a599-22d762243cea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr)\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)\n",
    "\n",
    "print(\"\\n\")\n",
    "print (\"Area Under Curve: %.2f\" %auc(fpr, tpr))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5965cdbe-6ffb-4322-8617-157da6f6a3b0"
   },
   "source": [
    "## Serialize Model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "043f90cc-db10-4008-b73c-1fb6968bc838",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(path, MODEL_NAME+\".pkl\")\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53014a01-a6c1-42bf-b6ad-8093315f3477",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(model_path,'wb') as f:\n",
    "    pickle.dump(model_pipeline,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76fafaa8-a2a7-4a3b-9c21-d30ec024b60c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cpd_ver = facts_client.get_cpd_version()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbbcaf7c-0b55-480b-bf59-952ef0947dd4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "WML_CREDENTIALS = {\n",
    "   \"token\": TOKEN,\n",
    "   \"instance_id\" : \"openshift\",\n",
    "   \"url\": os.environ['RUNTIME_ENV_APSX_URL'],\n",
    "   \"version\": \"4.6\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cea0c573-d210-489f-8434-96f01f2245b8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "wml_client = APIClient(WML_CREDENTIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e58f6bdd-7b1e-4a2e-bdeb-0d2a5751cfbb"
   },
   "source": [
    "## Save and Log Models in AI Factsheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rework this without helper scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec4da754-9d94-441a-9f40-cb4db2c15566"
   },
   "source": [
    "## Save Params in WS Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56e15da2-fcc3-4680-ab2c-f03f9a432ac5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_params = {}\n",
    "train_params['auc_roc'] = float(auc(fpr, tpr))\n",
    "train_params['training_done'] = True\n",
    "train_params['model_name'] = MODEL_NAME\n",
    "train_params['model_path'] = model_path\n",
    "train_params['deployment_name'] = DEPLOYMENT_NAME\n",
    "# train_params['model_id'] = model_id\n",
    "train_params['project_id'] = project_id if not None else None\n",
    "# train_params['model_pipeline'] = \"/home/wsuser/work/model_pipeline.pkl\"\n",
    "\n",
    "pipelines_client = WSPipelines.from_token(TOKEN)\n",
    "pipelines_client.store_results(train_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
