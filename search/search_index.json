{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is MLOps? \u00b6 Machine Learning Operations (MLOps) is a practice that builds on DevOps to create an automated and streamlined workflow specifically tailored for developing, deploying, and managing machine learning models and all associated assets. Machine learning models require extra consideration for monitoring models in production and retraining if the performance declines. Planning for MLOps and implementing a strategy is a step in the AI journey if a company wants to derive greater benefit from machine learning models and manage risk. Follow us as we provide a roadmap to becoming an AI -driven company. The engineering practice of MLOps leverages three contributing disciplines: machine learning, software engineering (especially DevOps), and data engineering. The goal of MLOps is to bridge the gap between development (Dev) and operations (Ops) and create a repeatable process for training, deploying, monitoring, and updating machine learning models. MLOps improves the collaboration between the different stakeholders in a data science project. An AI project might include data scientists, software engineers, and subject matter experts, among others. MLOps provides tools and processes so that collaborators can contribute their part to achieve a seamless flow from model request to deploying models to solve business problems. IBM is here to help with your MLOps journey and support you in gaining a better understanding of how MLOps works and how IBM products can be used to build a solid MLOps ecosystem for your organization. IBM's Data and AI portfolio includes the tools and capabilities you need to facilitate successful MLOps. Explore the information on this website to learn more about how to plan, execute, and customize an MLOps process that works for your needs. MLOps with IBM Cloud Pak for Data \u00b6 What steps and services to consider when realizing MLOps with IBMs Cloud Pak for Data? Follow the realizing MLOps steps in this Guide Follow the official documentation . About this Repository \u00b6 This GitHub repository has been created by several MLOps experts with expertise in IBM Cloud Pak for Data. Use this guide to learn how to design an MLOps strategy based on Cloud Pak for Data services. What would be a example definition of MLOps? \u00b6 MLOps (Machine Learning Operations) is a paradigm aimed at bridging the gap between development (Dev) and operations (Ops) to manage and automate the AI lifecycle. A full defintion of MLOps can be found e.g. here MLOps Paper . The latest version can always be found here: https://ibm.github.io/MLOps/ Download the latest (PDF) Download the latest (DOCX)","title":"What is MLOps?"},{"location":"#what-is-mlops","text":"Machine Learning Operations (MLOps) is a practice that builds on DevOps to create an automated and streamlined workflow specifically tailored for developing, deploying, and managing machine learning models and all associated assets. Machine learning models require extra consideration for monitoring models in production and retraining if the performance declines. Planning for MLOps and implementing a strategy is a step in the AI journey if a company wants to derive greater benefit from machine learning models and manage risk. Follow us as we provide a roadmap to becoming an AI -driven company. The engineering practice of MLOps leverages three contributing disciplines: machine learning, software engineering (especially DevOps), and data engineering. The goal of MLOps is to bridge the gap between development (Dev) and operations (Ops) and create a repeatable process for training, deploying, monitoring, and updating machine learning models. MLOps improves the collaboration between the different stakeholders in a data science project. An AI project might include data scientists, software engineers, and subject matter experts, among others. MLOps provides tools and processes so that collaborators can contribute their part to achieve a seamless flow from model request to deploying models to solve business problems. IBM is here to help with your MLOps journey and support you in gaining a better understanding of how MLOps works and how IBM products can be used to build a solid MLOps ecosystem for your organization. IBM's Data and AI portfolio includes the tools and capabilities you need to facilitate successful MLOps. Explore the information on this website to learn more about how to plan, execute, and customize an MLOps process that works for your needs.","title":"What is MLOps?"},{"location":"#mlops-with-ibm-cloud-pak-for-data","text":"What steps and services to consider when realizing MLOps with IBMs Cloud Pak for Data? Follow the realizing MLOps steps in this Guide Follow the official documentation .","title":"MLOps with IBM Cloud Pak for Data"},{"location":"#about-this-repository","text":"This GitHub repository has been created by several MLOps experts with expertise in IBM Cloud Pak for Data. Use this guide to learn how to design an MLOps strategy based on Cloud Pak for Data services.","title":"About this Repository"},{"location":"#what-would-be-a-example-definition-of-mlops","text":"MLOps (Machine Learning Operations) is a paradigm aimed at bridging the gap between development (Dev) and operations (Ops) to manage and automate the AI lifecycle. A full defintion of MLOps can be found e.g. here MLOps Paper . The latest version can always be found here: https://ibm.github.io/MLOps/ Download the latest (PDF) Download the latest (DOCX)","title":"What would be a example definition of MLOps?"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: autoai \u00b6 AutoAI (no-code)* SPSS Modeler (low-code)* cicd \u00b6 AI Governance AutoAI (no-code)* Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines SPSS Modeler (low-code)* Watson Studio continuous-delivery \u00b6 AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio git \u00b6 AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio gitops \u00b6 AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio ml \u00b6 Watson Machine Learning model_development \u00b6 Watson Machine Learning model_training \u00b6 AutoAI (no-code)* SPSS Modeler (low-code)* Watson Machine Learning","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#autoai","text":"AutoAI (no-code)* SPSS Modeler (low-code)*","title":"autoai"},{"location":"tags/#cicd","text":"AI Governance AutoAI (no-code)* Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines SPSS Modeler (low-code)* Watson Studio","title":"cicd"},{"location":"tags/#continuous-delivery","text":"AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio","title":"continuous-delivery"},{"location":"tags/#git","text":"AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio","title":"git"},{"location":"tags/#gitops","text":"AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio","title":"gitops"},{"location":"tags/#ml","text":"Watson Machine Learning","title":"ml"},{"location":"tags/#model_development","text":"Watson Machine Learning","title":"model_development"},{"location":"tags/#model_training","text":"AutoAI (no-code)* SPSS Modeler (low-code)* Watson Machine Learning","title":"model_training"},{"location":"MLOps%20Example%20Templates/examples/","text":"CP4D MLOps Example Templates \u00b6 For example, the CP4D MLOps Accelerator is a step-by-step guide to help you set-up your Cloud Pak for Data environment to faciliate efficient MLOps. The accelerator documents the implementation of a simple end-to-end MLOps workflow that uses services from the Cloud Pak for Data software stack to demonstrate the full AI lifecycle, from training to production. The accelerator is specifically built to demonstrate the creation of a basic workflow, while allowing for rapid customization. For example, you can tailor this flow to include your custom-built PyTorch or TensorFlow models. Use this accelerator as a guide to building your own MLOps pipeline. We include: full documentation of the CP4D environment set-up. sample notebooks that populate the environment with MLOps steps, including data_validation , model_training , and model_deployment . a pipeline that connects the standalone notebooks sequentially to create a repeatable, automated end-to-end flow. Additional Examples \u00b6 The sample mlops-sustainability-oss \ud83c\udf31 demonstrates a self-sustaining end-to-end CP4D MLOps workflow for a flood forecasting model with automated data/model versioning and rollback. More Examples (coming soon) \u00b6 We are continously improving our assets. More example assets will be added shortly.","title":"Examples"},{"location":"MLOps%20Example%20Templates/examples/#cp4d-mlops-example-templates","text":"For example, the CP4D MLOps Accelerator is a step-by-step guide to help you set-up your Cloud Pak for Data environment to faciliate efficient MLOps. The accelerator documents the implementation of a simple end-to-end MLOps workflow that uses services from the Cloud Pak for Data software stack to demonstrate the full AI lifecycle, from training to production. The accelerator is specifically built to demonstrate the creation of a basic workflow, while allowing for rapid customization. For example, you can tailor this flow to include your custom-built PyTorch or TensorFlow models. Use this accelerator as a guide to building your own MLOps pipeline. We include: full documentation of the CP4D environment set-up. sample notebooks that populate the environment with MLOps steps, including data_validation , model_training , and model_deployment . a pipeline that connects the standalone notebooks sequentially to create a repeatable, automated end-to-end flow.","title":"CP4D MLOps Example Templates"},{"location":"MLOps%20Example%20Templates/examples/#additional-examples","text":"The sample mlops-sustainability-oss \ud83c\udf31 demonstrates a self-sustaining end-to-end CP4D MLOps workflow for a flood forecasting model with automated data/model versioning and rollback.","title":"Additional Examples"},{"location":"MLOps%20Example%20Templates/examples/#more-examples-coming-soon","text":"We are continously improving our assets. More example assets will be added shortly.","title":"More Examples (coming soon)"},{"location":"Realizing%20MLOps/AI_Gov_OpenPages_Factsheets/","tags":["gitops","cicd","continuous-delivery","git"],"text":"AI Governance \u00b6 AI governance is about ensuring greater transparency across the AI lifecycle and the model itself. IBM recently announced watsonx.governance, a next generation enterprise toolkit which is designed to automate and accelerate workloads across the AI lifecycle while providing risk management and facilitating regulatory compliance. Use IBM AI Governance services to accelerate responsible, transparent, and explainable AI workflows with an AI governance solution that provides end-to-end monitoring for machine learning. Monitor machine learning assets from request to production. Collect facts about models that are built with IBM tools or third-party providers in a single dashboard to aid in meeting compliance and governance goals. Implement an approval workflow to meet complaince goals. These Cloud Pak for data services can be used individually or together as part of your governance and MLOps plan. AI Factsheets allows automated collection and documentation of model metadata at all stages, from model idea to production Model and process metadata is captured in a central meta store. Having all model facts in central place is important both to increase the productivity of the MLOps process (model facts are immediately visible to all parties involved in the lifecycle of an AI model) and to comply with regulatory requirements. Data scientists benefit from assistance and automation of the documentation process. Transparency of model metadata supports audits and brings more clarity to stakeholder or customer requests. Metadata captured in AI factsheets includes model details, training information, metrics, input and output schemas, or details about the models used, such as quality metrics, fairness or drift details. AI Factsheets - Official Documentation AI Governance - Factsheets - Official Documentation OpenPages : Govern models through the complete AI workflow considering policies and regulations The next generation governance-toolkit provides a range of capabilities to identify, manage, monitor, and report on risk and compliance. It accelerates the creation of models at scale, from use case idea (model candidates) to production deployment, by incorporating approvals in the workflow-based approach. Full transparency of any type of model (e.g., task specific data science artefacts or foundation models) is ensured and made visible in customisable risk monitoring dashboards. Additionally in Open Pages corporate policies and regulations can be assigned to models, e.g., annual bias review (required for EU AI ACT) to ensure that models are fair, transparent, and compliant [4]. OpenPages - Official Documentation OpenScale allows to monitor, explain, and benchmark your model Model monitoring is an ongoing task to track models and to drive transparency. This includes the monitoring of the general model performance (e.g., accuracy) and more specifically monitoring of fairness or model and data consistency over time (i.e. drift). Open Pages supports threshold definitions for model performance metrics and combines those with automated detection of threshold violations to trigger model retraining. It implements explainability by supporting explanations how the model arrived at certain predictions. Model benchmarking is supported \u2013 it is common practice to compare and benchmark a challenger model with a model in production to ensure that the best model is the one in production. See OpenScale in this Handbook . For more information check out the official documentation or the example Notebooks .","title":"AI Governance"},{"location":"Realizing%20MLOps/AI_Gov_OpenPages_Factsheets/#ai-governance","text":"AI governance is about ensuring greater transparency across the AI lifecycle and the model itself. IBM recently announced watsonx.governance, a next generation enterprise toolkit which is designed to automate and accelerate workloads across the AI lifecycle while providing risk management and facilitating regulatory compliance. Use IBM AI Governance services to accelerate responsible, transparent, and explainable AI workflows with an AI governance solution that provides end-to-end monitoring for machine learning. Monitor machine learning assets from request to production. Collect facts about models that are built with IBM tools or third-party providers in a single dashboard to aid in meeting compliance and governance goals. Implement an approval workflow to meet complaince goals. These Cloud Pak for data services can be used individually or together as part of your governance and MLOps plan. AI Factsheets allows automated collection and documentation of model metadata at all stages, from model idea to production Model and process metadata is captured in a central meta store. Having all model facts in central place is important both to increase the productivity of the MLOps process (model facts are immediately visible to all parties involved in the lifecycle of an AI model) and to comply with regulatory requirements. Data scientists benefit from assistance and automation of the documentation process. Transparency of model metadata supports audits and brings more clarity to stakeholder or customer requests. Metadata captured in AI factsheets includes model details, training information, metrics, input and output schemas, or details about the models used, such as quality metrics, fairness or drift details. AI Factsheets - Official Documentation AI Governance - Factsheets - Official Documentation OpenPages : Govern models through the complete AI workflow considering policies and regulations The next generation governance-toolkit provides a range of capabilities to identify, manage, monitor, and report on risk and compliance. It accelerates the creation of models at scale, from use case idea (model candidates) to production deployment, by incorporating approvals in the workflow-based approach. Full transparency of any type of model (e.g., task specific data science artefacts or foundation models) is ensured and made visible in customisable risk monitoring dashboards. Additionally in Open Pages corporate policies and regulations can be assigned to models, e.g., annual bias review (required for EU AI ACT) to ensure that models are fair, transparent, and compliant [4]. OpenPages - Official Documentation OpenScale allows to monitor, explain, and benchmark your model Model monitoring is an ongoing task to track models and to drive transparency. This includes the monitoring of the general model performance (e.g., accuracy) and more specifically monitoring of fairness or model and data consistency over time (i.e. drift). Open Pages supports threshold definitions for model performance metrics and combines those with automated detection of threshold violations to trigger model retraining. It implements explainability by supporting explanations how the model arrived at certain predictions. Model benchmarking is supported \u2013 it is common practice to compare and benchmark a challenger model with a model in production to ensure that the best model is the one in production. See OpenScale in this Handbook . For more information check out the official documentation or the example Notebooks .","title":"AI Governance"},{"location":"Realizing%20MLOps/autoai/","tags":["autoai","model_training","cicd"],"text":"AutoAI (no-code)* \u00b6 * Use of AutoAI remains optional for your MLOps workflow. Depending on your scenario it may increase efficiency and model quality. With AutoAI, you can train and save machine learning models without coding. The AutoAI tool in CP4D does most of the work for you. AutoAI uses sophisticated training capabilities to build models from your structured data sets. AutoAI automatically runs all major tasks that are part of building and ranking candidate model pipelines: Data pre-processing : AutoAI analyzes, cleans, and prepares your raw data for machine learning by applying various algorithms, or estimators. Unlike standard machine learning algorithms , it can work with various data formats and is able to handle missing values with data imputation methods you configure. Automated model selection : AutoAI analyzes the training data, then suggests the model type(binary classification, multiclass classification, or regression) that best matches the data. If you are working with sequenced data/time data, you can also configure the model selection to create a time-series model. Automated feature engineering : AutoAI explores various feature construction choices while progressively improving model accuracy with reinforcement learning to transform the data into the combination of features that best represents the problem for the most accurate prediction. Hyperparameter optimization : The AutoAI hyper-parameter optimization uses a novel hyperparameter optimization algorithm to rapidly find and apply optimizations for the model-candidate pipelines. Integrating AutoAI in your MLOps workflow \u00b6 The way you use AutoAI can vary depending on your use case. For example, you can use AutoAI to fully train models for your scenario. You can also use AutoAI to complement or accelerate your data science, by using it for specific tasks, such as model selection or hyperparameter optimization. As part of your MLOps planning, consider one of these approaches: If one of the above is true, or a similar scenario occurs, you are faced with two options. Full Reliance : You can implement AutoAI within the Watson Studio Pipeline of your end-to-end MLOps workflow as sole Model Selector, Model Trainer etc. Semi Reliance : Alternatively, you can run AutoAI in parallel to your custom train_model.ipynb notebook. In that case, your model training notebook will train your custom-built model while AutoAI trains all pre-developed models. You can then proceed to either deploy your custom-built model, or the model selected by AutoAI, depending on performance metrics you established earlier. For more information check out the official documentation . Examples official documentation .","title":"AutoAI (no-code)*"},{"location":"Realizing%20MLOps/autoai/#autoai-no-code","text":"* Use of AutoAI remains optional for your MLOps workflow. Depending on your scenario it may increase efficiency and model quality. With AutoAI, you can train and save machine learning models without coding. The AutoAI tool in CP4D does most of the work for you. AutoAI uses sophisticated training capabilities to build models from your structured data sets. AutoAI automatically runs all major tasks that are part of building and ranking candidate model pipelines: Data pre-processing : AutoAI analyzes, cleans, and prepares your raw data for machine learning by applying various algorithms, or estimators. Unlike standard machine learning algorithms , it can work with various data formats and is able to handle missing values with data imputation methods you configure. Automated model selection : AutoAI analyzes the training data, then suggests the model type(binary classification, multiclass classification, or regression) that best matches the data. If you are working with sequenced data/time data, you can also configure the model selection to create a time-series model. Automated feature engineering : AutoAI explores various feature construction choices while progressively improving model accuracy with reinforcement learning to transform the data into the combination of features that best represents the problem for the most accurate prediction. Hyperparameter optimization : The AutoAI hyper-parameter optimization uses a novel hyperparameter optimization algorithm to rapidly find and apply optimizations for the model-candidate pipelines.","title":"AutoAI (no-code)*"},{"location":"Realizing%20MLOps/autoai/#integrating-autoai-in-your-mlops-workflow","text":"The way you use AutoAI can vary depending on your use case. For example, you can use AutoAI to fully train models for your scenario. You can also use AutoAI to complement or accelerate your data science, by using it for specific tasks, such as model selection or hyperparameter optimization. As part of your MLOps planning, consider one of these approaches: If one of the above is true, or a similar scenario occurs, you are faced with two options. Full Reliance : You can implement AutoAI within the Watson Studio Pipeline of your end-to-end MLOps workflow as sole Model Selector, Model Trainer etc. Semi Reliance : Alternatively, you can run AutoAI in parallel to your custom train_model.ipynb notebook. In that case, your model training notebook will train your custom-built model while AutoAI trains all pre-developed models. You can then proceed to either deploy your custom-built model, or the model selected by AutoAI, depending on performance metrics you established earlier. For more information check out the official documentation . Examples official documentation .","title":"Integrating AutoAI in your MLOps workflow"},{"location":"Realizing%20MLOps/datarefinery_datastage/","tags":["gitops","cicd","continuous-delivery","git"],"text":"Data Stage and Data Refinery \u00b6 Data Stage Data Refinery With the DataStage data integration tool, data engineers can design and run complex ETL data pipelines that move and transform data between operational, transactional, and analytical target systems. Data integration specialists use DataStage to develop flows that process and transform data. DataStage has capabilities that allow connecting directly to enterprise applications, cloud data sources, relational and NoSQL systems, REST endpoints, and more. You can administer, manage, deploy, and reuse these flows to integrate data across many systems throughout your organization. For more information check out the official documentation . Data Refinery involves transforming raw data into clean, structured information for analysis and decision-making. It includes data collection, cleansing, integration, enrichment, and validation. Data is gathered from various sources, cleaned to remove errors, and integrated into a unified format. Enrichment adds additional context and attributes. Validation ensures data quality. Data refinery unlocks the value of data, enabling informed decisions and actionable insights. For more information check out the official documentation . Refine your data in Data Refinery official documentation .","title":"Data Stage and Data Refinery"},{"location":"Realizing%20MLOps/datarefinery_datastage/#data-stage-and-data-refinery","text":"Data Stage Data Refinery With the DataStage data integration tool, data engineers can design and run complex ETL data pipelines that move and transform data between operational, transactional, and analytical target systems. Data integration specialists use DataStage to develop flows that process and transform data. DataStage has capabilities that allow connecting directly to enterprise applications, cloud data sources, relational and NoSQL systems, REST endpoints, and more. You can administer, manage, deploy, and reuse these flows to integrate data across many systems throughout your organization. For more information check out the official documentation . Data Refinery involves transforming raw data into clean, structured information for analysis and decision-making. It includes data collection, cleansing, integration, enrichment, and validation. Data is gathered from various sources, cleaned to remove errors, and integrated into a unified format. Enrichment adds additional context and attributes. Validation ensures data quality. Data refinery unlocks the value of data, enabling informed decisions and actionable insights. For more information check out the official documentation . Refine your data in Data Refinery official documentation .","title":"Data Stage and Data Refinery"},{"location":"Realizing%20MLOps/openscale/","tags":["gitops","cicd","continuous-delivery","git"],"text":"Watson OpenScale \u00b6 What is Watson OpenScale? How we utilize the power of Watson OpenScale After you deploy a machine learning model, the work doesn\u2019t stop. To guarantee that the model is functioning in production as expected, you must have a plan for monitoring the deployed model and updating it as needed. As part of your end-to-end MLOps process, consider IBM Watson OpenScale to evaluate model deployment to make sure they are fair, accurate, and performing to your standards.\u2022 When OpenScale is installed or provisioned as part of your Cloud Pak suite, you can provide the details for a deployment, then run scheduled evaluations that measure dimensions you configure for thresholds you set. For example, if you want to test whether predicted outcomes are fair across various age groups, you can configure the Fairness monitor to evaluate the outcomes for a monitored group, such as young adults, and compare the results to the age group most likely to get favorable results. If the results deviate more than a threshold you specify, you will get an alert that results require attention. The dimensions you can test are: Fairness: Configure a monitor for fairness to check if your model produces biased results for different groups, like gender or race. Set thresholds to measure predictions for a monitored group compared to a reference group. Quality: Configure a monitor for quality to assess your model's performance based on labeled test data. Set quality thresholds to track when a metric value falls outside an acceptable range. Drift: Configure a monitor for drift to ensure your deployments are up-to-date and consistent. Use feature importance to determine the impact of feature drift on your model. Explainability: Configure explainability settings to understand which features influence your model's predictions. Different methods like SHAP and LIME are available to suit your needs. All of the evaluation results can be reviewed and monitored in a single dashboard, as shown in this example: For more information check out the official documentation or the example Notebooks . In this use case we use Watson OpenScale to automatically verify: whether the model is performing at a constant high accuracy. If performance dips below a threshold we set, an alert is triggered. whether the test data produces output that is similar to the training data. If there is a noticable deviation, we are alerted that it might be time to retrain the model. whether the model is discriminating against a particular group. In this case, we look at outcomes for older customers to ensure they are being treated fairly as compared to the reference group. Results are automatically checked by Watson OpenScale whenever new data is scored by the model. If one of the checks fails, an alert will be send to the responsible person so that steps can be taken to mitigate the problem. OpenScale also generates visualizations that you can use to diagnose a potential problem, as shown in this example:","title":"Watson OpenScale"},{"location":"Realizing%20MLOps/openscale/#watson-openscale","text":"What is Watson OpenScale? How we utilize the power of Watson OpenScale After you deploy a machine learning model, the work doesn\u2019t stop. To guarantee that the model is functioning in production as expected, you must have a plan for monitoring the deployed model and updating it as needed. As part of your end-to-end MLOps process, consider IBM Watson OpenScale to evaluate model deployment to make sure they are fair, accurate, and performing to your standards.\u2022 When OpenScale is installed or provisioned as part of your Cloud Pak suite, you can provide the details for a deployment, then run scheduled evaluations that measure dimensions you configure for thresholds you set. For example, if you want to test whether predicted outcomes are fair across various age groups, you can configure the Fairness monitor to evaluate the outcomes for a monitored group, such as young adults, and compare the results to the age group most likely to get favorable results. If the results deviate more than a threshold you specify, you will get an alert that results require attention. The dimensions you can test are: Fairness: Configure a monitor for fairness to check if your model produces biased results for different groups, like gender or race. Set thresholds to measure predictions for a monitored group compared to a reference group. Quality: Configure a monitor for quality to assess your model's performance based on labeled test data. Set quality thresholds to track when a metric value falls outside an acceptable range. Drift: Configure a monitor for drift to ensure your deployments are up-to-date and consistent. Use feature importance to determine the impact of feature drift on your model. Explainability: Configure explainability settings to understand which features influence your model's predictions. Different methods like SHAP and LIME are available to suit your needs. All of the evaluation results can be reviewed and monitored in a single dashboard, as shown in this example: For more information check out the official documentation or the example Notebooks . In this use case we use Watson OpenScale to automatically verify: whether the model is performing at a constant high accuracy. If performance dips below a threshold we set, an alert is triggered. whether the test data produces output that is similar to the training data. If there is a noticable deviation, we are alerted that it might be time to retrain the model. whether the model is discriminating against a particular group. In this case, we look at outcomes for older customers to ensure they are being treated fairly as compared to the reference group. Results are automatically checked by Watson OpenScale whenever new data is scored by the model. If one of the checks fails, an alert will be send to the responsible person so that steps can be taken to mitigate the problem. OpenScale also generates visualizations that you can use to diagnose a potential problem, as shown in this example:","title":"Watson OpenScale"},{"location":"Realizing%20MLOps/overview/","tags":["gitops","cicd","continuous-delivery","git"],"text":"Overview - Realizing MLOps \u00b6 IBM Cloud Pak for Data provides a full stack of tools to assist you in realizing your MLOps strategy. The services work together to provide all of the capabilities you need to work with data, create AI assets, deploy the assets for productive use, and monitor and manage deployed assets. Note: You do not have to use the full list of Cloud Pak for Data services listed here. You can choose the services you need to realize your MLOps solution. Key Cloud Pak for Data services for implementing MLOps include: Data Stage and Data Refinery for integrating and preparing data to train machine learning models. Watson Studio for organizing and creating assets, including data, models, and notebooks. \"+ experiment tracking in Factsheets after each run.\" Watson Machine Learning for organizing the resources, including environments, for deploying and managing machine learning models, scripts, and functions. Watson Machine Learning includes also: AutoAI (no-code) for automating training of machine learning models (for example, data scientists can use AutoAI to rapidly prototype a solution.) SPSS Modeler (low-code) build an ML model by dragging and dropping operators and assets on a canvas and running a model as a flow. Watson Pipelines for automating a repeatable flow to manage the assets in your MLOps pipeline. Key Cloud Pak for Data services for implementing the AI Governance part of your MLOps strategy include: Watson OpenScale to evaluate and monitor deployments to ensure they perform to expectations for such dimensions as fairness, quality, and drift. OpenPages manage your workflows by meeting governance regulations, policies, regulations for your assets (models) and build trust in your solution. AI Factsheets captures model metadata, in all stages, from request to production. The MLOps suite of services all provide rich user interfaces in Cloud Pak for Data for building and managing assets. For example, monitor all of your deployed assets from a Deployments dashboard in Watson Machine Learning, or assemble and run your MLOps flow from a Watson Pipelines canvas. Alternatively, you can use programming interfaces to code MLOps assets and processes. Watson Machine Learning provides these programming interfaces: Use the Python client library to work with all of your Watson Machine Learning assets in a notebook. Use the REST API to call methods from the base URLs for the Watson Machine Learning API endpoints.","title":"Overview - Realizing MLOps"},{"location":"Realizing%20MLOps/overview/#overview-realizing-mlops","text":"IBM Cloud Pak for Data provides a full stack of tools to assist you in realizing your MLOps strategy. The services work together to provide all of the capabilities you need to work with data, create AI assets, deploy the assets for productive use, and monitor and manage deployed assets. Note: You do not have to use the full list of Cloud Pak for Data services listed here. You can choose the services you need to realize your MLOps solution. Key Cloud Pak for Data services for implementing MLOps include: Data Stage and Data Refinery for integrating and preparing data to train machine learning models. Watson Studio for organizing and creating assets, including data, models, and notebooks. \"+ experiment tracking in Factsheets after each run.\" Watson Machine Learning for organizing the resources, including environments, for deploying and managing machine learning models, scripts, and functions. Watson Machine Learning includes also: AutoAI (no-code) for automating training of machine learning models (for example, data scientists can use AutoAI to rapidly prototype a solution.) SPSS Modeler (low-code) build an ML model by dragging and dropping operators and assets on a canvas and running a model as a flow. Watson Pipelines for automating a repeatable flow to manage the assets in your MLOps pipeline. Key Cloud Pak for Data services for implementing the AI Governance part of your MLOps strategy include: Watson OpenScale to evaluate and monitor deployments to ensure they perform to expectations for such dimensions as fairness, quality, and drift. OpenPages manage your workflows by meeting governance regulations, policies, regulations for your assets (models) and build trust in your solution. AI Factsheets captures model metadata, in all stages, from request to production. The MLOps suite of services all provide rich user interfaces in Cloud Pak for Data for building and managing assets. For example, monitor all of your deployed assets from a Deployments dashboard in Watson Machine Learning, or assemble and run your MLOps flow from a Watson Pipelines canvas. Alternatively, you can use programming interfaces to code MLOps assets and processes. Watson Machine Learning provides these programming interfaces: Use the Python client library to work with all of your Watson Machine Learning assets in a notebook. Use the REST API to call methods from the base URLs for the Watson Machine Learning API endpoints.","title":"Overview - Realizing MLOps"},{"location":"Realizing%20MLOps/pipelines/","tags":["gitops","cicd","continuous-delivery","git"],"text":"Watson Pipelines \u00b6 With the Watson\u2122 Pipelines service, you can create a pipeline to automate the end-to-end flow of various assets, whether it\u2019s training a model or running script-based assets from the time they are created through their deployment. Automating the end-to-end flow of these assets with a pipeline makes it simpler to build, run, and evaluate models, which speeds up the flow and reduces the overall time investment. You use a pipelines editor canvas to assemble and configure a pipeline that creates, trains, deploys, and updates machine learning models and Python scripts. To design a pipeline, you drag nodes onto the canvas, specify objects and parameters, then run and monitor the pipeline. Note that you must have the required service for any asset you include in your pipeline. For example, if you are cleaning data with DataStage, the DataStage service must be installed or provisioned for your Cloud Pak suite. Your team can collaborate across roles in the pipelines editor. For example, a data scientist can create a flow to train a model in the editor, and then a ModelOps engineer can add the steps to the flow to automate the process of training, deploying, and evaluating the model to a production environment. After you assemble the pipeline, you can rapidly update and test modifications with the Pipelines editor canvas, which provides tools to visualize the pipeline, customize it at run time with pipeline parameter variables, and then run it as a trial job or on a schedule. These tools are available with the Watson Pipelines service: Create a flow to collect data, run scripts, train models, store results, and more. Customize a unique pipelines component that can run a user-written function. Schedule jobs to run flows and enhance automation by adding node conditions. For more information check out the official documentation . Example Use [official documentation] ( https://www.ibm.com/docs/en/cloud-paks/cp-data/4.7.x?topic=assets-watson-pipelines ).","title":"Watson Pipelines"},{"location":"Realizing%20MLOps/pipelines/#watson-pipelines","text":"With the Watson\u2122 Pipelines service, you can create a pipeline to automate the end-to-end flow of various assets, whether it\u2019s training a model or running script-based assets from the time they are created through their deployment. Automating the end-to-end flow of these assets with a pipeline makes it simpler to build, run, and evaluate models, which speeds up the flow and reduces the overall time investment. You use a pipelines editor canvas to assemble and configure a pipeline that creates, trains, deploys, and updates machine learning models and Python scripts. To design a pipeline, you drag nodes onto the canvas, specify objects and parameters, then run and monitor the pipeline. Note that you must have the required service for any asset you include in your pipeline. For example, if you are cleaning data with DataStage, the DataStage service must be installed or provisioned for your Cloud Pak suite. Your team can collaborate across roles in the pipelines editor. For example, a data scientist can create a flow to train a model in the editor, and then a ModelOps engineer can add the steps to the flow to automate the process of training, deploying, and evaluating the model to a production environment. After you assemble the pipeline, you can rapidly update and test modifications with the Pipelines editor canvas, which provides tools to visualize the pipeline, customize it at run time with pipeline parameter variables, and then run it as a trial job or on a schedule. These tools are available with the Watson Pipelines service: Create a flow to collect data, run scripts, train models, store results, and more. Customize a unique pipelines component that can run a user-written function. Schedule jobs to run flows and enhance automation by adding node conditions. For more information check out the official documentation . Example Use [official documentation] ( https://www.ibm.com/docs/en/cloud-paks/cp-data/4.7.x?topic=assets-watson-pipelines ).","title":"Watson Pipelines"},{"location":"Realizing%20MLOps/spss_modeller/","tags":["autoai","model_training","cicd"],"text":"SPSS Modeler (low-code)* \u00b6 * Use of SPSS Modeler remains optional for your MLOps workflow. Depending on your scenario it may increase efficiency and model quality. You can use SPSS Modeler flows to build machine learning pipelines that you can use to iterate rapidly during the model building process. Whether you're trying to find the best algorithm or experimenting with different ways of preparing your data, you can create reproducible research that's easily understood by any member of your team. With SPSS Modeler flows, you can quickly develop predictive models using business expertise to improve decision making. The flows interface is based on the long-established SPSS Modeler client software and uses industry-standard CRISP-DM methodology. The SPSS Modeler service supports the entire data mining process, from data exploration all the way to better business results. For more information check out the official documentation . Examples official documentation .","title":"SPSS Modeler (low-code)*"},{"location":"Realizing%20MLOps/spss_modeller/#spss-modeler-low-code","text":"* Use of SPSS Modeler remains optional for your MLOps workflow. Depending on your scenario it may increase efficiency and model quality. You can use SPSS Modeler flows to build machine learning pipelines that you can use to iterate rapidly during the model building process. Whether you're trying to find the best algorithm or experimenting with different ways of preparing your data, you can create reproducible research that's easily understood by any member of your team. With SPSS Modeler flows, you can quickly develop predictive models using business expertise to improve decision making. The flows interface is based on the long-established SPSS Modeler client software and uses industry-standard CRISP-DM methodology. The SPSS Modeler service supports the entire data mining process, from data exploration all the way to better business results. For more information check out the official documentation . Examples official documentation .","title":"SPSS Modeler (low-code)*"},{"location":"Realizing%20MLOps/watsonstudio/","tags":["gitops","cicd","continuous-delivery","git"],"text":"Watson Studio \u00b6 Watson Studio is part of Cloud Pak for Data and provides the data science capabilities of the data fabric architecture. Watson Studio provides the environment and tools for you to collaboratively work on data to solve your business problems. You can choose the tools you need to analyze and visualize data, to cleanse and shape data, to ingest streaming data, or to create and train machine learning models. For more information check out the official documentation . Creating Jupyter Notebooks in Watson Studio official documentation .","title":"Watson Studio"},{"location":"Realizing%20MLOps/watsonstudio/#watson-studio","text":"Watson Studio is part of Cloud Pak for Data and provides the data science capabilities of the data fabric architecture. Watson Studio provides the environment and tools for you to collaboratively work on data to solve your business problems. You can choose the tools you need to analyze and visualize data, to cleanse and shape data, to ingest streaming data, or to create and train machine learning models. For more information check out the official documentation . Creating Jupyter Notebooks in Watson Studio official documentation .","title":"Watson Studio"},{"location":"Realizing%20MLOps/wml/","tags":["ml","model_training","model_development"],"text":"Watson Machine Learning \u00b6 Watson Machine Learning (WML) provides a range of tools and services for you to build, train, test, and deploy models in Cloud Pak for Data. Depending on what is installed and configured for your deployment, you can use Watson Machine Learning to: Build, train, and deploy models from notebooks by using the Watson Machine Learning Python client library or the Watson Machine Learning API . Create AutoAI experiments. AutoAI automatically preprocesses your structured data, selects the best estimator for the data, and then generates model candidate pipelines for you to review and compare. Deploy the best-performing pipeline as a machine learning model. Run experiments to train complex Deep Learning models in Experiment Builder. Deploy your models so that you can score the models and generate predictions. Evaluate online deployments (requires Watson OpenScale service). Training models with Watson Machine Learning \u00b6 Watson Machine Learning supports training machine learning models with tools that provide automation or autonomy matching your needs. Build a custom model using popular Machine Learning frameworks such as PyTorch or TensorFlow *), or fully automate model creation and training with AutoAI . AutoAI enables you to build and deploy machine learning models without coding. Use it as a rapid prototyping tool or as part of your end-to-end machine learning solutions. You can rely on the auto-detection features that analyze the training data, select a model type, and apply algorithms and tuning features, or you can customize the configuration to exercise finer control. Save the best model-candidate pipeline as a deployable model, or save the model code as a notebook so you can review the code and customaize as needed. AutoAI can be a powerful part of your machine learning solution. Deploying models and other assets with Watson Machine Learning \u00b6 Using IBM Watson Machine Learning, you can collect and organize all of the dependencies required to bring a a model, script, function or web app from training to production. For example, create a pre-production collaboratve space where you can upload the deployable asset and testing data for validating a deployment before moving it to a production space and putting it to work predicting outcomes based on real-world input data. Manging Deployments - Official documentation Extending Watson Machine Learning to govern machine learning assets \u00b6 As part of your ModelOps strategy, you can extend Watson Machine Learning to include governance features, provided with the Watson OpenScale service. Once your tools and services are connected, you can monitor and evaluate online deployments for dimensions such as fairness, quality, and drift. Machine learning models require vigilance to make sure they do not stray from their intended mission. Part of the MLOps pipeline is to measure the machine learning outcomes and retrain and update the model and deployment when performance or outcomes fall below the thresholds you establish. Watson OpenScale also provides tools for creating what-if scenarios to better understand how a model is performing or to find better approaches to solving the business problem. Finally, track and collect the metadata for assets as they move through the AI lifecycle to ensure compliance with governance standard. The combination of Watson Machine Learning and Watson OpenScale gives you the power to create AI solutions that are effective, responsible, and trustworthy.","title":"Watson Machine Learning"},{"location":"Realizing%20MLOps/wml/#watson-machine-learning","text":"Watson Machine Learning (WML) provides a range of tools and services for you to build, train, test, and deploy models in Cloud Pak for Data. Depending on what is installed and configured for your deployment, you can use Watson Machine Learning to: Build, train, and deploy models from notebooks by using the Watson Machine Learning Python client library or the Watson Machine Learning API . Create AutoAI experiments. AutoAI automatically preprocesses your structured data, selects the best estimator for the data, and then generates model candidate pipelines for you to review and compare. Deploy the best-performing pipeline as a machine learning model. Run experiments to train complex Deep Learning models in Experiment Builder. Deploy your models so that you can score the models and generate predictions. Evaluate online deployments (requires Watson OpenScale service).","title":"Watson Machine Learning"},{"location":"Realizing%20MLOps/wml/#training-models-with-watson-machine-learning","text":"Watson Machine Learning supports training machine learning models with tools that provide automation or autonomy matching your needs. Build a custom model using popular Machine Learning frameworks such as PyTorch or TensorFlow *), or fully automate model creation and training with AutoAI . AutoAI enables you to build and deploy machine learning models without coding. Use it as a rapid prototyping tool or as part of your end-to-end machine learning solutions. You can rely on the auto-detection features that analyze the training data, select a model type, and apply algorithms and tuning features, or you can customize the configuration to exercise finer control. Save the best model-candidate pipeline as a deployable model, or save the model code as a notebook so you can review the code and customaize as needed. AutoAI can be a powerful part of your machine learning solution.","title":"Training models with Watson Machine Learning"},{"location":"Realizing%20MLOps/wml/#deploying-models-and-other-assets-with-watson-machine-learning","text":"Using IBM Watson Machine Learning, you can collect and organize all of the dependencies required to bring a a model, script, function or web app from training to production. For example, create a pre-production collaboratve space where you can upload the deployable asset and testing data for validating a deployment before moving it to a production space and putting it to work predicting outcomes based on real-world input data. Manging Deployments - Official documentation","title":"Deploying models and other assets with Watson Machine Learning"},{"location":"Realizing%20MLOps/wml/#extending-watson-machine-learning-to-govern-machine-learning-assets","text":"As part of your ModelOps strategy, you can extend Watson Machine Learning to include governance features, provided with the Watson OpenScale service. Once your tools and services are connected, you can monitor and evaluate online deployments for dimensions such as fairness, quality, and drift. Machine learning models require vigilance to make sure they do not stray from their intended mission. Part of the MLOps pipeline is to measure the machine learning outcomes and retrain and update the model and deployment when performance or outcomes fall below the thresholds you establish. Watson OpenScale also provides tools for creating what-if scenarios to better understand how a model is performing or to find better approaches to solving the business problem. Finally, track and collect the metadata for assets as they move through the AI lifecycle to ensure compliance with governance standard. The combination of Watson Machine Learning and Watson OpenScale gives you the power to create AI solutions that are effective, responsible, and trustworthy.","title":"Extending Watson Machine Learning to govern machine learning assets"},{"location":"contributors/contributors/","text":"Contributors \u00b6 This MLOps Guide has been created by several different technical communities: Customer Success, Client Engineering, Expert Labs, Documentation Team, Consulting, Development and Infrastructure. Special thanks to our Executive Champions Kate Blair and Hardy Gr\u00f6ger for supporting this initiative! Name IBM Unit/Description Hardy Gr\u00f6ger Distinguished Engineer & Technical Lead Data and AI DACH Kate Blair Director, Product Management, watsonx.ai Dominik Kreuzberger Customer Success Maximilan Jesch Customer Success Hasan \u00d6zdemir Customer Success Carsten Holtmann Customer Success Ilias Ennmouri Customer Success Yvette Machowski Customer Success Alejandro Gonz\u00e1lez Customer Success Daniel Horn Customer Success Andrea Barto\u0161 Customer Success Benedikt Bothur Customer Success Ivan Iliash Customer Success Moritz Scheele Expert Labs Lena Eckstein Expert Labs Lucas Baier Client Engineering Leonard Brucksch Client Engineering Michael V\u00f6ssing Client Engineering Ellen Hoeven Client Engineering Tim Noah-Schmidt Client Engineering Michael Daschner Development Mihai Criveti Consulting Maximilian Wurzer Consulting Sebastian Lehrig Infrastructure Julianne Forgo Documentation Team - Data & AI Przemek Czuba Documentation Team - Data & AI","title":"Contributors"},{"location":"contributors/contributors/#contributors","text":"This MLOps Guide has been created by several different technical communities: Customer Success, Client Engineering, Expert Labs, Documentation Team, Consulting, Development and Infrastructure. Special thanks to our Executive Champions Kate Blair and Hardy Gr\u00f6ger for supporting this initiative! Name IBM Unit/Description Hardy Gr\u00f6ger Distinguished Engineer & Technical Lead Data and AI DACH Kate Blair Director, Product Management, watsonx.ai Dominik Kreuzberger Customer Success Maximilan Jesch Customer Success Hasan \u00d6zdemir Customer Success Carsten Holtmann Customer Success Ilias Ennmouri Customer Success Yvette Machowski Customer Success Alejandro Gonz\u00e1lez Customer Success Daniel Horn Customer Success Andrea Barto\u0161 Customer Success Benedikt Bothur Customer Success Ivan Iliash Customer Success Moritz Scheele Expert Labs Lena Eckstein Expert Labs Lucas Baier Client Engineering Leonard Brucksch Client Engineering Michael V\u00f6ssing Client Engineering Ellen Hoeven Client Engineering Tim Noah-Schmidt Client Engineering Michael Daschner Development Mihai Criveti Consulting Maximilian Wurzer Consulting Sebastian Lehrig Infrastructure Julianne Forgo Documentation Team - Data & AI Przemek Czuba Documentation Team - Data & AI","title":"Contributors"},{"location":"products/cp4d/","text":"Cloud Pak for Data \u00b6 IBM Cloud Pak for Data is a cloud-native platform that helps you to build, deploy, and manage machine learning models at scale. It provides a unified set of tools and services for the entire MLOps lifecycle, from data preparation to model deployment and monitoring. Features \u00b6 Centralized data catalog: Makes it easy to find and share data. Self-service data preparation tool: Helps you to clean and prepare your data for analysis. Machine learning platform: Makes it easy to build and deploy machine learning models. Model registry: Tracks the lineage and performance of your models. Model monitoring tool: Helps you to track the performance of your models in production. Benefits \u00b6 Reduces the time it takes to get your models into production. Improves the accuracy and performance of your models. Helps you to make better decisions based on your models. Saves money on your ML infrastructure costs. Conclusion \u00b6 IBM Cloud Pak for Data is a great option for organizations that are looking to accelerate their MLOps journey. It is a powerful and flexible platform that can help you to build, deploy, and manage machine learning models at scale.","title":"Cloud Pak for Data"},{"location":"products/cp4d/#cloud-pak-for-data","text":"IBM Cloud Pak for Data is a cloud-native platform that helps you to build, deploy, and manage machine learning models at scale. It provides a unified set of tools and services for the entire MLOps lifecycle, from data preparation to model deployment and monitoring.","title":"Cloud Pak for Data"},{"location":"products/cp4d/#features","text":"Centralized data catalog: Makes it easy to find and share data. Self-service data preparation tool: Helps you to clean and prepare your data for analysis. Machine learning platform: Makes it easy to build and deploy machine learning models. Model registry: Tracks the lineage and performance of your models. Model monitoring tool: Helps you to track the performance of your models in production.","title":"Features"},{"location":"products/cp4d/#benefits","text":"Reduces the time it takes to get your models into production. Improves the accuracy and performance of your models. Helps you to make better decisions based on your models. Saves money on your ML infrastructure costs.","title":"Benefits"},{"location":"products/cp4d/#conclusion","text":"IBM Cloud Pak for Data is a great option for organizations that are looking to accelerate their MLOps journey. It is a powerful and flexible platform that can help you to build, deploy, and manage machine learning models at scale.","title":"Conclusion"},{"location":"products/watsonx/","text":"Extend your AI capabilities with watsonx \u00b6 IBM watsonx.ai is a studio of integrated tools for working with generative AI capabilities that are powered by foundation models and for building machine learning models. IBM watsonx.ai provides a secure and collaborative environment where you can access your organization's trusted data, automate AI processes, and deliver AI in your applications. You can accomplish the following goals with IBM watsonx.ai: Build machine learning models by using open source frameworks and code-based, automated, or visual data science tools. Experiment with foundation models using prompts to pre-trained foundation models to generate, classify, summarize, or extract content from your input text. Choose from IBM models or open source models from Hugging Face. Manage the AI lifecycle and automate the full AI model lifecycle with all the integrated tools and runtimes to train, validate, and deploy AI models. Relationship with Cloud Pak for Data \u00b6 IBM watsonx as a Service and Cloud Pak for Data as a Service have similar platform functionality and are compatible in many ways. The watsonx platform provides a subset of the tools and services that are provided by Cloud Pak for Data as a Service. However, watsonx.ai on watsonx extends the functionality of the common toolset to enable working with foundation models and generative AI . Both platforms provide services for data science and MLOps use cases: Watson Studio Watson Machine Learning AI governance However, these services for watsonx.ai on the watsonx platform include features for working with foundation models and generative AI that are not included in these services on Cloud Pak for Data as a Service. Cloud Pak for Data as a Service also provides services for these use cases: Data integration Data governance Data science and AI tools \u00b6 Both platforms provide a common set of data science and AI tools. However, on watsonx, you can also perform foundation model inferencing with the Prompt Lab tool or with a Python library in notebooks. Foundation model inferencing and the Prompt Lab tool are not available on Cloud Pak for Data. The following table shows which data science and AI tools are available on each platform. Tool On watsonx On Cloud Pak for Data Prompt Lab \u2713 Synthetic Data Generator \u2713 Prompt lab \u2713 Synthetic Data Generator \u2713 Data Refinery \u2713 \u2713 Visualizations \u2713 \u2713 Jupyter notebooks \u2713 \u2713 Federated Learning \u2713 \u2713 RStudio IDE \u2713 \u2713 SPSS Modeler \u2713 \u2713 Decision Optimization \u2713 \u2713 AutoAI \u2713 \u2713 Watson Pipelines \u2713 \u2713 Which one should you use? The best platform for you will depend on your specific needs and requirements. If you are looking for a platform that can help you to multiply the impact of AI across your organization, then watsonx is a good option. If you are looking for a platform that can help you to manage your data and build AI applications, then Cloud Pak for Data is a good option. For details on watsonx, including how you can try this technology, see watsonx . Ultimately, the best way to decide which platform is right for you is to evaluate your needs and requirements and then speak with an IBM representative.","title":"Extend your AI capabilities with watsonx"},{"location":"products/watsonx/#extend-your-ai-capabilities-with-watsonx","text":"IBM watsonx.ai is a studio of integrated tools for working with generative AI capabilities that are powered by foundation models and for building machine learning models. IBM watsonx.ai provides a secure and collaborative environment where you can access your organization's trusted data, automate AI processes, and deliver AI in your applications. You can accomplish the following goals with IBM watsonx.ai: Build machine learning models by using open source frameworks and code-based, automated, or visual data science tools. Experiment with foundation models using prompts to pre-trained foundation models to generate, classify, summarize, or extract content from your input text. Choose from IBM models or open source models from Hugging Face. Manage the AI lifecycle and automate the full AI model lifecycle with all the integrated tools and runtimes to train, validate, and deploy AI models.","title":"Extend your AI capabilities with watsonx"},{"location":"products/watsonx/#relationship-with-cloud-pak-for-data","text":"IBM watsonx as a Service and Cloud Pak for Data as a Service have similar platform functionality and are compatible in many ways. The watsonx platform provides a subset of the tools and services that are provided by Cloud Pak for Data as a Service. However, watsonx.ai on watsonx extends the functionality of the common toolset to enable working with foundation models and generative AI . Both platforms provide services for data science and MLOps use cases: Watson Studio Watson Machine Learning AI governance However, these services for watsonx.ai on the watsonx platform include features for working with foundation models and generative AI that are not included in these services on Cloud Pak for Data as a Service. Cloud Pak for Data as a Service also provides services for these use cases: Data integration Data governance","title":"Relationship with Cloud Pak for Data"},{"location":"products/watsonx/#data-science-and-ai-tools","text":"Both platforms provide a common set of data science and AI tools. However, on watsonx, you can also perform foundation model inferencing with the Prompt Lab tool or with a Python library in notebooks. Foundation model inferencing and the Prompt Lab tool are not available on Cloud Pak for Data. The following table shows which data science and AI tools are available on each platform. Tool On watsonx On Cloud Pak for Data Prompt Lab \u2713 Synthetic Data Generator \u2713 Prompt lab \u2713 Synthetic Data Generator \u2713 Data Refinery \u2713 \u2713 Visualizations \u2713 \u2713 Jupyter notebooks \u2713 \u2713 Federated Learning \u2713 \u2713 RStudio IDE \u2713 \u2713 SPSS Modeler \u2713 \u2713 Decision Optimization \u2713 \u2713 AutoAI \u2713 \u2713 Watson Pipelines \u2713 \u2713 Which one should you use? The best platform for you will depend on your specific needs and requirements. If you are looking for a platform that can help you to multiply the impact of AI across your organization, then watsonx is a good option. If you are looking for a platform that can help you to manage your data and build AI applications, then Cloud Pak for Data is a good option. For details on watsonx, including how you can try this technology, see watsonx . Ultimately, the best way to decide which platform is right for you is to evaluate your needs and requirements and then speak with an IBM representative.","title":"Data science and AI tools"},{"location":"watsonx/watsonx/","text":"Introducing watsonx \u00b6 Machine Learning Operations (MLOps) is the process of moving machine learning models from development and testing to production. MLOps is now extended to support training of foundation models and putting foundation model assets into productive use. Foundational models are the basis of generative AI . Large Language Models (LLMs) are a type of foundation models that support language-based generative AI used for a variety of applications, including advanced chat or summarization. A large language model takes input in the form of a prompt and generates output. Operating large language models in production is MLOps for LLMs called LLMOps. Overview of watsonx: For the latest news on watsonx offerings and capabilities, see https://www.ibm.com/watsonx .","title":"Introducing watsonx"},{"location":"watsonx/watsonx/#introducing-watsonx","text":"Machine Learning Operations (MLOps) is the process of moving machine learning models from development and testing to production. MLOps is now extended to support training of foundation models and putting foundation model assets into productive use. Foundational models are the basis of generative AI . Large Language Models (LLMs) are a type of foundation models that support language-based generative AI used for a variety of applications, including advanced chat or summarization. A large language model takes input in the form of a prompt and generates output. Operating large language models in production is MLOps for LLMs called LLMOps. Overview of watsonx: For the latest news on watsonx offerings and capabilities, see https://www.ibm.com/watsonx .","title":"Introducing watsonx"},{"location":"tags/","text":"Tags \u00b6 Following is a list of relevant tags: autoai \u00b6 AutoAI (no-code)* SPSS Modeler (low-code)* cicd \u00b6 AI Governance AutoAI (no-code)* Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines SPSS Modeler (low-code)* Watson Studio continuous-delivery \u00b6 AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio git \u00b6 AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio gitops \u00b6 AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio ml \u00b6 Watson Machine Learning model_development \u00b6 Watson Machine Learning model_training \u00b6 AutoAI (no-code)* SPSS Modeler (low-code)* Watson Machine Learning","title":"Tags"},{"location":"tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"tags/#autoai","text":"AutoAI (no-code)* SPSS Modeler (low-code)*","title":"autoai"},{"location":"tags/#cicd","text":"AI Governance AutoAI (no-code)* Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines SPSS Modeler (low-code)* Watson Studio","title":"cicd"},{"location":"tags/#continuous-delivery","text":"AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio","title":"continuous-delivery"},{"location":"tags/#git","text":"AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio","title":"git"},{"location":"tags/#gitops","text":"AI Governance Data Stage and Data Refinery Watson OpenScale Overview - Realizing MLOps Watson Pipelines Watson Studio","title":"gitops"},{"location":"tags/#ml","text":"Watson Machine Learning","title":"ml"},{"location":"tags/#model_development","text":"Watson Machine Learning","title":"model_development"},{"location":"tags/#model_training","text":"AutoAI (no-code)* SPSS Modeler (low-code)* Watson Machine Learning","title":"model_training"}]}