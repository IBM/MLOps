{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "## Data Preparation Notebook"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_selection import SelectKBest\n",
                "from sklearn.feature_selection import chi2\n",
                "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder,LabelEncoder,MinMaxScaler\n",
                "from sklearn.feature_selection import mutual_info_classif\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from botocore.client import Config\n",
                "from ibm_watson_studio_pipelines import WSPipelines\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# import itc_utils.flight_service as itcfs\n",
                "\n",
                "import heapq\n",
                "import pickle\n",
                "import os, types\n",
                "import pandas as pd\n",
                "import ibm_boto3"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load the validated training and test data from IBM Filesystem (alternatively Db2) \n",
                "\n",
                "```\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pipeline Params"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "TOKEN = os.getenv(\"USER_ACCESS_TOKEN\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data_path = os.getenv(\"train_data_path\")\n",
                "test_data_path = os.getenv(\"test_data_path\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipeline_path = os.getenv(\"pipeline_path\")\n",
                "if pipeline_path is None:\n",
                "    pipeline_path = \"feature_encode.pickle\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data_path = \"mlops-dir/train_gcr.csv\"\n",
                "test_data_path = \"mlops-dir/test_gcr.csv\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###  Read and Write Utility"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_data_in_filesystem(df,filename):\n",
                "    \"\"\"\n",
                "    Save Data in Filesystem\n",
                "\n",
                "    Passed filename should involve path\n",
                "\n",
                "    \"\"\"\n",
                "    try:\n",
                "        if filename[-3:] == \"csv\":\n",
                "            df.to_csv(filename,index=False)\n",
                "            print(f\"File {filename} persisted successfully as csv\")\n",
                "        else:\n",
                "            with open(filename, 'wb') as f:\n",
                "                pickle.dump(df, f)\n",
                "            print(f\"File {filename} pickled successfully\")\n",
                "    except Exception as e:\n",
                "        print(e)\n",
                "        print(f\"File serialization for {filename} failed\")\n",
                "\n",
                "def check_for_file_in_filesystem(path):\n",
                "    \"\"\"\n",
                "    Check existence of path in filesystem\n",
                "    \"\"\"\n",
                "    if os.path.exists(path):\n",
                "        return True\n",
                "    else:\n",
                "        return False\n",
                "    \n",
                "def read_data_from_db2(data_request):\n",
                "    read_client = itcfs.get_flight_client()\n",
                "    DB2_DATA_data_request = {\n",
                "        'connection_name': \"\"\"DB2_DATA\"\"\",\n",
                "        'interaction_properties': {\n",
                "            'select_statement': 'SELECT * FROM \"CUSTOMER_DATA\".\"GERMAN_CREDIT_RISK_TRAINING\" FETCH FIRST 5000 ROWS ONLY'\n",
                "        }\n",
                "    }\n",
                "\n",
                "    flightInfo = itcfs.get_flight_info(read_client, nb_data_request=data_request)\n",
                "\n",
                "    df = itcfs.read_pandas_and_concat(read_client, flightInfo, timeout=240)\n",
                "    return df\n",
                "    \n",
                "def load_data_from_filesystem(path):\n",
                "    \"\"\"\n",
                "    Check existence of path in filesystem.\n",
                "    If it does exist, loads csv via path\n",
                "    If it does NOT exist, try to load data from Db2\n",
                "    \"\"\"\n",
                "    body = check_for_file_in_filesystem(path)\n",
                "    if body:\n",
                "        suffix = path[-3:]\n",
                "        # Check whether path ends on csv\n",
                "        if suffix == \"csv\":\n",
                "            gcf_df = pd.read_csv(path)\n",
                "        else:\n",
                "            with open(path) as f:\n",
                "                gcf_df = pickle.load(f)\n",
                "\n",
                "        return gcf_df\n",
                "    else:\n",
                "        print(\"\\n\")\n",
                "        print(f\"{path} file/path is probably not in project. Loading File from MLOps COS Bucket.\")\n",
                "\n",
                "        data_request = {\n",
                "                'connection_name': \"\"\"DB2_DATA\"\"\",\n",
                "                'interaction_properties': {\n",
                "                    'select_statement': 'SELECT * FROM \"CUSTOMER_DATA\".\"GERMAN_CREDIT_RISK_TRAINING\" FETCH FIRST 5000 ROWS ONLY'\n",
                "                }\n",
                "            }\n",
                "\n",
                "        gcf_df = read_data_from_db2(data_request)\n",
                "        return gcf_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train_Data "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[39m=\u001b[39m load_data_from_filesystem(train_data_path)\n\u001b[1;32m      2\u001b[0m train_data\u001b[39m.\u001b[39mhead()\n",
                        "Cell \u001b[0;32mIn[6], line 49\u001b[0m, in \u001b[0;36mload_data_from_filesystem\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data_from_filesystem\u001b[39m(path):\n\u001b[1;32m     44\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m    Check existence of path in filesystem.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m    If it does exist, loads csv via path\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m    If it does NOT exist, try to load data from Db2\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     body \u001b[39m=\u001b[39m check_for_file_in_filesystem(path)\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m body:\n\u001b[1;32m     51\u001b[0m         suffix \u001b[39m=\u001b[39m path[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:]\n",
                        "Cell \u001b[0;32mIn[6], line 24\u001b[0m, in \u001b[0;36mcheck_for_file_in_filesystem\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_for_file_in_filesystem\u001b[39m(path):\n\u001b[1;32m     21\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m    Check existence of path in filesystem\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mexists(path):\n\u001b[1;32m     25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
                        "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[1;32m     20\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
                        "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
                    ]
                }
            ],
            "source": [
                "train_data = load_data_from_filesystem(train_data_path)\n",
                "train_data.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "object_df = train_data.select_dtypes('O')\n",
                "object_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "object_cols = list(set(object_df.columns.tolist()) - set(['Risk']))\n",
                "object_cols"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numerical_columns = [col for col in train_data.columns.tolist() if col not in object_cols and col!='Risk']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Test Data "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_data = load_data_from_filesystem(test_data_path)\n",
                "test_data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Split X and Y "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_train = train_data['Risk']\n",
                "X_train = train_data.drop(\"Risk\",axis=1)\n",
                "\n",
                "\n",
                "y_test = test_data['Risk']\n",
                "X_test = test_data.drop(\"Risk\",axis=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Categorcial Feature Analysis "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_input_data(X_train, X_test):\n",
                "    oe = OrdinalEncoder()\n",
                "    oe.fit(X_train)\n",
                "    X_train_enc = oe.transform(X_train)\n",
                "    X_test_enc = oe.transform(X_test)\n",
                "    return X_train_enc, X_test_enc\n",
                "\n",
                "\n",
                "def prepare_output_data(y_train, y_test):\n",
                "    le = LabelEncoder()\n",
                "    le.fit(y_train)\n",
                "    y_train_enc = le.transform(y_train)\n",
                "    y_test_enc = le.transform(y_test)\n",
                "    return y_train_enc, y_test_enc\n",
                "\n",
                "\n",
                "def select_best_chi2_features(X_train, y_train, X_test,score_func=chi2):\n",
                "    featureselector = SelectKBest(score_func=chi2, k='all')\n",
                "    featureselector.fit(X_train, y_train)\n",
                "    X_train_best_feat = featureselector.transform(X_train)\n",
                "    X_test_best_feat= featureselector.transform(X_test)\n",
                "    return X_train_best_feat, X_test_best_feat, featureselector\n",
                "\n",
                "\n",
                "def select_best_mutualinf_features(X_train, y_train, X_test,k=5):\n",
                "    featureselector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
                "    featureselector.fit(X_train, y_train)\n",
                "    X_train_best_feat = fs.transform(X_train)\n",
                "    X_test_best_feat= fs.transform(X_test)\n",
                "    return X_train_best_feat, X_test_best_feat, featureselector\n",
                "\n",
                "# def plot_scores():\n",
                "#     plt.figure(figsize=(14, 12))\n",
                "#     plt.subplot(221)\n",
                "\n",
                "#     ax1 = sns.barplot([i for i in range(len(fs.scores_))], fs.scores_)\n",
                "#     ax1.set_title(\"Chi2 Importance Scores\", fontsize=20)\n",
                "#     ax1.set_xlabel(\"Features\",fontsize=15)\n",
                "#     ax1.set_ylabel(\"Chi2 Scores\",fontsize=15)\n",
                "    \n",
                "    \n",
                "def get_top_k_catgeorical(fs,train_cat,k=10):\n",
                "    fs_score_map = {}\n",
                "    for i in range(len(fs.scores_)):\n",
                "        #print(f\"Feature {train_cat.columns.tolist()[i]} {fs.scores_[i]}\")\n",
                "        fs_score_map[train_cat.columns.tolist()[i]] = fs.scores_[i]\n",
                "        \n",
                "    k_keys_sorted_by_values = heapq.nlargest(k, fs_score_map, key=fs_score_map.get)\n",
                "    \n",
                "    return k_keys_sorted_by_values"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Encode and shape the Variables "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train_enc, X_test_enc = prepare_input_data(X_train[object_cols], X_test[object_cols])\n",
                "\n",
                "y_train_enc, y_test_enc = prepare_output_data(y_train, y_test)\n",
                "\n",
                "X_train_fs, X_test_fs, fs = select_best_chi2_features(X_train_enc, y_train_enc, X_test_enc)\n",
                "\n",
                "# plot_scores()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Top K Categorical Features  based on Chi2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "top_k_cat = get_top_k_catgeorical(fs,X_train[object_cols])\n",
                "top_k_cat"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Top K Categorical Features  based on Mutual Information Feature Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train_enc_mf, X_test_enc_mf = prepare_input_data(X_train[object_cols], X_test[object_cols])\n",
                "\n",
                "y_train_enc_mf, y_test_enc_mf = prepare_output_data(y_train, y_test)\n",
                "\n",
                "X_train_fs_mf, X_test_fs_mf, fs_mf = select_best_chi2_features(X_train_enc_mf, y_train_enc_mf, X_test_enc_mf)\n",
                "\n",
                "# plot_scores()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "top_k_cat_mf = get_top_k_catgeorical(fs_mf,X_train[object_cols])\n",
                "top_k_cat_mf"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "union_features = list(set(top_k_cat+top_k_cat_mf))\n",
                "if \"Sex\" not in union_features:\n",
                "    union_features.append(\"Sex\")\n",
                "union_features"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Filter the Top K Categorical features and Merge to Original Train and Test Dataframes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train_object_filtered = X_train[union_features]\n",
                "X_test_object_filtered = X_test[union_features]\n",
                "\n",
                "X_train_final = pd.concat([X_train[numerical_columns],X_train_object_filtered],axis=1)\n",
                "\n",
                "X_test_final = pd.concat([X_test[numerical_columns],X_test_object_filtered],axis=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Use Column Transformer and Pipelines to encode the Input and Output Variables . Scale the Numerical columns using MinMaxScaler."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numerical_ix = X_train_final.select_dtypes(include=['int64', 'float64']).columns\n",
                "categorical_ix = X_train_final.select_dtypes(include=['object', 'bool']).columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoding_steps = [('cat', OrdinalEncoder(), categorical_ix), ('num', MinMaxScaler(), numerical_ix)]\n",
                "col_transform = ColumnTransformer(transformers=encoding_steps)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipeline = Pipeline(steps=[('prep',col_transform)])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_final = pd.concat([X_train_final,y_train],axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_final = pd.concat([X_test_final,y_test],axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#encoded_train = pd.DataFrame(pipeline.fit_transform(X_train_final),columns=X_train_final.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#encoded_test = pd.DataFrame(pipeline.transform(X_test_final),columns=X_test_final.columns)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save the Prepared Data to IBM COS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save_data_in_filesystem(df=train_final, filename=train_data_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save_data_in_filesystem(df=test_final, filename=test_data_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save_data_in_filesystem(df=pipeline, filename=pipeline_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check if files have been copied "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_prep_done = check_for_file_in_filesystem(train_data_path) and check_for_file_in_filesystem(test_data_path) and check_for_file_in_filesystem(pipeline_path)\n",
                "data_prep_done"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Store Params in WS Pipelines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "preparation_params = {}\n",
                "preparation_params['data_prep_done'] = data_prep_done\n",
                "preparation_params['pipeline_path'] = pipeline_path\n",
                "\n",
                "pipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\n",
                "pipelines_client.store_results(preparation_params)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "vscode": {
            "interpreter": {
                "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
