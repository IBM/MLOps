{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": [
                "## Connection and Data Validation Notebook"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Table of Contents\n",
                "* [Check for Training Data in Project Space](#DataCheck)\n",
                "    * [Load the Training Data from Db2 if it does not exist in the project space](#section_1_1)\n",
                "    \n",
                "    * [Check the connection and data loading](#section_1_2)\n",
                "  \n",
                "* [Data Validation](#chapter2)\n",
                "    * [Split the Data](#Optional)\n",
                "\n",
                "    * [Generate Training Stats on both Splits](#section_2_2)\n",
                "    * [Infer Schema on both Splits](#section_2_3) \n",
                "    * [Check for anomalies](#section_2_4) \n",
                "    * [Return a boolean to validate the tests](#section_3_1) \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from botocore.client import Config\n",
                "from sklearn.model_selection import train_test_split\n",
                "from dataclasses import dataclass\n",
                "\n",
                "# import itc_utils.flight_service as itcfs\n",
                "\n",
                "\n",
                "# TODO: Await (or build) for py3.10 version\n",
                "# import tensorflow_data_validation as tfdv\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "from ibm_watson_studio_pipelines import WSPipelines\n",
                "import ibm_boto3\n",
                "\n",
                "import logging\n",
                "import os, types\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load the Credentials\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These environment variables are set in WS Pipelines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "TOKEN = os.getenv(\"USER_ACCESS_TOKEN\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Expects existing path and existing training_file_name within that path\n",
                "training_file_name = os.getenv(\"training_file_name\")\n",
                "path = os.getenv(\"path\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_file_name = \"german_credit_data_biased_training.csv\"\n",
                "path = \"mlops-dir\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_file_path = os.path.join(path, training_file_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check for Training Data in Project Space"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def check_for_file_in_filesystem(path):\n",
                "    if os.path.exists(path):\n",
                "        return True\n",
                "    else:\n",
                "        return False\n",
                "    \n",
                "def read_data_from_db2(data_request):\n",
                "    read_client = itcfs.get_flight_client()\n",
                "    DB2_DATA_data_request = {\n",
                "        'connection_name': \"\"\"DB2_DATA\"\"\",\n",
                "        'interaction_properties': {\n",
                "            'select_statement': 'SELECT * FROM \"CUSTOMER_DATA\".\"GERMAN_CREDIT_RISK_TRAINING\" FETCH FIRST 5000 ROWS ONLY'\n",
                "        }\n",
                "    }\n",
                "\n",
                "    flightInfo = itcfs.get_flight_info(read_client, nb_data_request=data_request)\n",
                "\n",
                "    df = itcfs.read_pandas_and_concat(read_client, flightInfo, timeout=240)\n",
                "    return df\n",
                "    \n",
                "def load_data_from_project(path):\n",
                "    body = check_for_file_in_filesystem(path)\n",
                "    if body:\n",
                "        gcf_df = pd.read_csv(path)\n",
                "        return gcf_df\n",
                "    else:\n",
                "        print(\"\\n\")\n",
                "        print(f\"{path} file/path is probably not in project. Loading File from MLOps COS Bucket.\")\n",
                "\n",
                "        data_request = {\n",
                "                'connection_name': \"\"\"DB2_DATA\"\"\",\n",
                "                'interaction_properties': {\n",
                "                    'select_statement': 'SELECT * FROM \"CUSTOMER_DATA\".\"GERMAN_CREDIT_RISK_TRAINING\" FETCH FIRST 5000 ROWS ONLY'\n",
                "                }\n",
                "            }\n",
                "\n",
                "        gcf_df = read_data_from_db2(data_request)\n",
                "        return gcf_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load the Training Data from Db2 if the file doesn't exist"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gcr_df = load_data_from_project(training_file_path)\n",
                "\n",
                "## Encode for ease of use with OpenScale\n",
                "gcr_df['Risk'] = gcr_df['Risk'].map({'Risk':1,'No Risk':0})\n",
                "gcr_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Validation "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class Datavalidation:\n",
                "    \"\"\"\n",
                "    \n",
                "    Data Validation Class\n",
                "    \n",
                "    \"\"\"\n",
                "    dataframe : pd.DataFrame\n",
                "    mask_per :int\n",
                "    \n",
                "    \n",
                "    def split_data(self,seed=32):\n",
                "        \"\"\"\n",
                "        Split Data into Train and Test Splits\n",
                "        \n",
                "        \"\"\"\n",
                "        np.random.seed(seed)\n",
                "        mask = np.random.rand(len(self.dataframe)) <= self.mask_per\n",
                "        training_data = gcr_df[mask]\n",
                "        testing_data = gcr_df[~mask]\n",
                "\n",
                "        print(f\"No. of training examples: {training_data.shape[0]}\")\n",
                "        print(f\"No. of testing examples: {testing_data.shape[0]}\")\n",
                "        \n",
                "        return training_data, testing_data\n",
                "    \n",
                "    # TODO: Replace with Db2/fileystem\n",
                "    def save_data_in_filesystem(self,df,filename):\n",
                "        \"\"\"\n",
                "        Save Data in Filesystem\n",
                "\n",
                "        Passed filename should involve path\n",
                "\n",
                "        \"\"\"\n",
                "        try:\n",
                "            df.to_csv(filename,index=False)\n",
                "            print(f\"File {filename} persisted successfully\")\n",
                "        except Exception as e:\n",
                "            print(e)\n",
                "            print(f\"File serialization for {filename} failed\")\n",
                "    \n",
                "    def generate_statistics(self,df):\n",
                "        \"\"\"\n",
                "        \n",
                "        Generate Statistics on a given Dataframe\n",
                "        \n",
                "        \"\"\"\n",
                "        train_stats = tfdv.generate_statistics_from_dataframe(df)\n",
                "        tfdv.visualize_statistics(train_stats)\n",
                "        return train_stats\n",
                "    \n",
                "    def inferSchema(self,stats):\n",
                "        \n",
                "        \"\"\"\n",
                "        InferSchema on a given Dataframe\n",
                "        \n",
                "        \"\"\"\n",
                "        schema = tfdv.infer_schema(statistics=stats)\n",
                "        tfdv.display_schema(schema=schema)\n",
                "        return schema\n",
                "    \n",
                "    def compare_statistics(self,lhs,rhs):\n",
                "        \"\"\"\n",
                "        \n",
                "        Compare Statistics between a test dataframe and reference Schema\n",
                "        \n",
                "        \"\"\"\n",
                "        # Compare evaluation data with training data\n",
                "        tfdv.visualize_statistics(lhs_statistics=lhs, rhs_statistics=rhs,\n",
                "                                  lhs_name='TEST_DATASET', rhs_name='TRAIN_DATASET')\n",
                "        \n",
                "        \n",
                "    def check_for_anomalies(self,testable_stats,ref_schema):\n",
                "        \"\"\"\n",
                "        \n",
                "        Check for any anomalies based on statistics and schema and values\n",
                "        \n",
                "        \"\"\"\n",
                "        anomalies = tfdv.validate_statistics(statistics=testable_stats, schema=ref_schema)\n",
                "        tfdv.display_anomalies(anomalies)\n",
                "        if len(anomalies.anomaly_info.items()) > 0:\n",
                "            logger.error(\"Anomalies found in dataset...\")\n",
                "            logger.error(str(self.anomalies.anomaly_info.items()))\n",
                "            return True\n",
                "        else:\n",
                "            return False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###  Split Data into Train and Eval Splits to Check for Consistency"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "classvalidate = Datavalidation(dataframe=gcr_df,mask_per=0.8) \n",
                "\n",
                "training_data, testing_data = classvalidate.split_data()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate Training Stats on both Splits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_stats = classvalidate.generate_statistics(training_data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_stats = classvalidate.generate_statistics(testing_data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Infer Training Data Schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_schema = classvalidate.inferSchema(train_stats)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Infer Test Data Schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_schema = classvalidate.inferSchema(test_stats)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Compare Eval and Train Data "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "classvalidate.compare_statistics(lhs=test_stats,rhs=train_stats)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check For Data Anomalies "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Check eval data for errors by validating the eval data stats using the previously inferred schema."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "anomaly_status = classvalidate.check_for_anomalies(test_stats,train_schema)\n",
                "anomaly_status"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Train and Test Data for Data Preparation Stage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data_path = os.path.join(path, \"train_gcr.csv\")\n",
                "test_data_path = os.path.join(path, \"test_gcr.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "anomaly_status = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Replace with Db2/fileystem\n",
                "if not anomaly_status:\n",
                "    classvalidate.save_data_in_filesystem(df=training_data,filename=train_data_path)\n",
                "    classvalidate.save_data_in_filesystem(df=testing_data,filename=test_data_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check if files Exists in COS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Replace with Db2/fileystem\n",
                "files_copied_in_cos = check_for_file_in_filesystem(train_data_path) and check_for_file_in_filesystem(test_data_path)\n",
                "files_copied_in_cos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Register a Boolean Variable in WS Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "validation_params = {}\n",
                "validation_params['anomaly_status'] = anomaly_status\n",
                "validation_params['files_copied_in_cos'] = files_copied_in_cos\n",
                "validation_params['train_data_path'] = train_data_path\n",
                "validation_params['test_data_path'] = test_data_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pipelines_client = WSPipelines.from_token(token=TOKEN)\n",
                "pipelines_client = WSPipelines.from_apikey(apikey=CLOUD_API_KEY)\n",
                "pipelines_client.store_results(validation_params)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "vscode": {
            "interpreter": {
                "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
