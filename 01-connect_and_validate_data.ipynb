{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9501e8a8-d435-4451-a8ae-513e984aafe9"
   },
   "source": [
    "## Connection and Data Validation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b64a45cb-07f3-4a20-a863-6a17d9fb17c9"
   },
   "source": [
    "## Table of Contents\n",
    "* [Check for Training Data in Project Space](#DataCheck)\n",
    "    * [Load the Training Data from Db2 if it does not exist in the project space](#section_1_1)\n",
    "    \n",
    "    * [Check the connection and data loading](#section_1_2)\n",
    "  \n",
    "* [Data Validation](#chapter2)\n",
    "    * [Split the Data](#Optional)\n",
    "\n",
    "    * [Generate Training Stats on both Splits](#section_2_2)\n",
    "    * [Infer Schema on both Splits](#section_2_3) \n",
    "    * [Check for anomalies](#section_2_4) \n",
    "    * [Return a boolean to validate the tests](#section_3_1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1f3c063-b384-4b66-8087-854e8e76d88c"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4ae4550-4c6a-40e7-b3bc-6ef29ad96164"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-data-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9c0bf685-3210-4506-bb32-80265e832a22"
   },
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dc1650f-d3fd-49f3-820d-dbce4ab98d04"
   },
   "outputs": [],
   "source": [
    "# this is a workaround as long as there are issues with custom python enviroments in multiple projects\n",
    "!pip install ibm_watson_studio_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fe00539c-a0ab-4769-ba7b-805adea59cf8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 07:24:33.933101: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-23 07:24:33.933161: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-23 07:24:33.933204: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-23 07:24:35.469552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from botocore.client import Config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import itc_utils.flight_service as itcfs\n",
    "\n",
    "\n",
    "# TODO: Await (or build) for py3.10 version\n",
    "import tensorflow_data_validation as tfdv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ibm_watson_studio_pipelines import WSPipelines\n",
    "\n",
    "import logging\n",
    "import os, types\n",
    "import warnings\n",
    "import requests\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "062a2e71-ddbf-4c77-aaeb-d5aa7ac8d265"
   },
   "source": [
    "### Load the Credentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63ab6462-e420-4aea-93ee-4e0b80de3c9e"
   },
   "source": [
    "These environment variables are set in WS Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e875ff36-3235-43fb-8008-4bfb334c1325",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOKEN = os.getenv(\"USER_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "665c7cd8-2611-4fea-b679-f47b7b293d7f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_file_name = \"german_credit_data_biased_training.csv\"\n",
    "mlops_path = \"mlops-dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3b2eb9c-b905-4ac4-a8de-8ca7de8f7521"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(mlops_path):\n",
    "    print(\"Path exists\")\n",
    "else:\n",
    "    # create path\n",
    "    os.mkdir(mlops_path)\n",
    "    # download training data csv from https://raw.githubusercontent.com/IBM/monitor-wml-model-with-watson-openscale/master/data/german_credit_data_biased_training.csv\n",
    "    url = \"https://raw.githubusercontent.com/IBM/monitor-wml-model-with-watson-openscale/master/data/german_credit_data_biased_training.csv\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        with open(\"german_credit_data_biased_training.csv\", \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(\"Downloaded and saved as 'german_credit_data_biased_training.csv'\")\n",
    "    else:\n",
    "        print(\"Failed to download the CSV file. Status code:\", response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b343151-c20f-41ba-b7b2-6cea3e9ca42a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_file_path = os.path.join(mlops_path, training_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d25efd1-cabd-4721-bb9d-461875a977db"
   },
   "source": [
    "## Check for Training Data in Project Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "b249d03b-a811-4624-9f71-003991cb1832",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_for_file_in_filesystem(path):\n",
    "    if os.path.exists(path):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def read_data_from_db2(data_request):\n",
    "    read_client = itcfs.get_flight_client()\n",
    "    DB2_DATA_data_request = {\n",
    "        'connection_name': \"\"\"DB2_DATA\"\"\",\n",
    "        'interaction_properties': {\n",
    "            'select_statement': 'SELECT * FROM \"CUSTOMER_DATA\".\"GERMAN_CREDIT_RISK_TRAINING\" FETCH FIRST 5000 ROWS ONLY'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    flightInfo = itcfs.get_flight_info(read_client, nb_data_request=data_request)\n",
    "\n",
    "    df = itcfs.read_pandas_and_concat(read_client, flightInfo, timeout=240)\n",
    "    return df\n",
    "    \n",
    "def load_data_from_project(path):\n",
    "    body = check_for_file_in_filesystem(path)\n",
    "    if body:\n",
    "        gcf_df = pd.read_csv(path)\n",
    "        return gcf_df\n",
    "    else:\n",
    "        print(\"\\n\")\n",
    "        print(f\"{path} file/path is probably not in project. Loading File from MLOps COS Bucket.\")\n",
    "\n",
    "        data_request = {\n",
    "                'connection_name': \"\"\"DB2_DATA\"\"\",\n",
    "                'interaction_properties': {\n",
    "                    'select_statement': 'SELECT * FROM \"CUSTOMER_DATA\".\"GERMAN_CREDIT_RISK_TRAINING\" FETCH FIRST 5000 ROWS ONLY'\n",
    "                }\n",
    "            }\n",
    "\n",
    "        gcf_df = read_data_from_db2(data_request)\n",
    "        return gcf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d44e72ca-25cf-44c2-a8f4-4ad6eff50e4a"
   },
   "source": [
    "## Load the Training Data from Db2 if the file doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0487c397-3d75-4292-ae6f-8f2cd4bb8f19",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gcr_df \u001b[38;5;241m=\u001b[39m load_data_from_project(\u001b[43mtraining_file_path\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## Encode for ease of use with OpenScale\u001b[39;00m\n\u001b[1;32m      4\u001b[0m gcr_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRisk\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gcr_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRisk\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRisk\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo Risk\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "gcr_df = load_data_from_project(training_file_path)\n",
    "\n",
    "## Encode for ease of use with OpenScale\n",
    "gcr_df['Risk'] = gcr_df['Risk'].map({'Risk':1,'No Risk':0})\n",
    "gcr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e38d5859-3df7-4174-8f8c-a047c0dcdb3c"
   },
   "source": [
    "## Data Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37d30747-1f1d-42ad-ab50-6001740df627",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Datavalidation:\n",
    "    \"\"\"\n",
    "    \n",
    "    Data Validation Class\n",
    "    \n",
    "    \"\"\"\n",
    "    dataframe : pd.DataFrame\n",
    "    mask_per :int\n",
    "    \n",
    "    \n",
    "    def split_data(self,seed=32):\n",
    "        \"\"\"\n",
    "        Split Data into Train and Test Splits\n",
    "        \n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        mask = np.random.rand(len(self.dataframe)) <= self.mask_per\n",
    "        training_data = gcr_df[mask]\n",
    "        testing_data = gcr_df[~mask]\n",
    "\n",
    "        print(f\"No. of training examples: {training_data.shape[0]}\")\n",
    "        print(f\"No. of testing examples: {testing_data.shape[0]}\")\n",
    "        \n",
    "        return training_data, testing_data\n",
    "    \n",
    "    # TODO: Replace with Db2/fileystem\n",
    "    def save_data_in_filesystem(self,df,filename):\n",
    "        \"\"\"\n",
    "        Save Data in Filesystem\n",
    "\n",
    "        Passed filename should involve path\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df.to_csv(filename,index=False)\n",
    "            print(f\"File {filename} persisted successfully\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"File serialization for {filename} failed\")\n",
    "    \n",
    "    def generate_statistics(self,df):\n",
    "        \"\"\"\n",
    "        \n",
    "        Generate Statistics on a given Dataframe\n",
    "        \n",
    "        \"\"\"\n",
    "        train_stats = tfdv.generate_statistics_from_dataframe(df)\n",
    "        tfdv.visualize_statistics(train_stats)\n",
    "        return train_stats\n",
    "    \n",
    "    def inferSchema(self,stats):\n",
    "        \n",
    "        \"\"\"\n",
    "        InferSchema on a given Dataframe\n",
    "        \n",
    "        \"\"\"\n",
    "        schema = tfdv.infer_schema(statistics=stats)\n",
    "        tfdv.display_schema(schema=schema)\n",
    "        return schema\n",
    "    \n",
    "    def compare_statistics(self,lhs,rhs):\n",
    "        \"\"\"\n",
    "        \n",
    "        Compare Statistics between a test dataframe and reference Schema\n",
    "        \n",
    "        \"\"\"\n",
    "        # Compare evaluation data with training data\n",
    "        tfdv.visualize_statistics(lhs_statistics=lhs, rhs_statistics=rhs,\n",
    "                                  lhs_name='TEST_DATASET', rhs_name='TRAIN_DATASET')\n",
    "        \n",
    "        \n",
    "    def check_for_anomalies(self,testable_stats,ref_schema):\n",
    "        \"\"\"\n",
    "        \n",
    "        Check for any anomalies based on statistics and schema and values\n",
    "        \n",
    "        \"\"\"\n",
    "        anomalies = tfdv.validate_statistics(statistics=testable_stats, schema=ref_schema)\n",
    "        tfdv.display_anomalies(anomalies)\n",
    "        if len(anomalies.anomaly_info.items()) > 0:\n",
    "            logger.error(\"Anomalies found in dataset...\")\n",
    "            logger.error(str(self.anomalies.anomaly_info.items()))\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fff2bb05-abd0-4e0a-9727-d1127a617f57"
   },
   "source": [
    "###  Split Data into Train and Eval Splits to Check for Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65163ff9-3193-4837-ab55-d66de3a5076f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "classvalidate = Datavalidation(dataframe=gcr_df,mask_per=0.8) \n",
    "\n",
    "training_data, testing_data = classvalidate.split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75029b2c-6341-4a59-957c-cbb2d33e3e39"
   },
   "source": [
    "## Generate Training Stats on both Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36a97f3f-3cd7-493d-8a4b-c3c87ae0710f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_stats = classvalidate.generate_statistics(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "020788e5-00ba-4779-bdb7-398445bfb7fc"
   },
   "outputs": [],
   "source": [
    "test_stats = classvalidate.generate_statistics(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed4d3f56-e852-4269-a3dd-8426d71bed8e"
   },
   "source": [
    "## Infer Training Data Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13f7be93-3fb1-49ca-9238-ebb1fcc1af28"
   },
   "outputs": [],
   "source": [
    "train_schema = classvalidate.inferSchema(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e79f56a-29b6-4c80-a201-16fe5eb5a245"
   },
   "source": [
    "## Infer Test Data Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ce74ad2-9ec9-48f3-959a-f0b59b077706"
   },
   "outputs": [],
   "source": [
    "test_schema = classvalidate.inferSchema(test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea9f07ac-ac4b-4ecd-8840-a5ab07bfb7f8"
   },
   "source": [
    "## Compare Eval and Train Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5ccfb5d-05bd-4b3b-9f4d-8082252457c3"
   },
   "outputs": [],
   "source": [
    "classvalidate.compare_statistics(lhs=test_stats,rhs=train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "915f6a20-aab3-47b5-86e9-93bde2f548ff"
   },
   "source": [
    "## Check For Data Anomalies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eae32b5f-46b3-4e5d-919e-68c206a47b8e"
   },
   "source": [
    "### Check eval data for errors by validating the eval data stats using the previously inferred schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2927d8b-fa0f-420f-9181-a080cdfcb748"
   },
   "outputs": [],
   "source": [
    "anomaly_status = classvalidate.check_for_anomalies(test_stats,train_schema)\n",
    "anomaly_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c914eade-25d9-497a-960e-ec5a57282def"
   },
   "source": [
    "## Save Train and Test Data for Data Preparation Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e508e03-07c4-4a22-bdba-dc8fc400eca8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_filename = \"train_gcr.csv\"\n",
    "test_data_filename = \"test_gcr.csv\"\n",
    "train_data_path = os.path.join(path, train_data_filename)\n",
    "test_data_path = os.path.join(path, test_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b0d34de-d26e-460b-8bbd-66c4cf603e2b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomaly_status = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f1d08cc-e4f7-4010-8b4e-7414529e874a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Replace with Db2/fileystem\n",
    "if not anomaly_status:\n",
    "    classvalidate.save_data_in_filesystem(df=training_data,filename=train_data_path)\n",
    "    classvalidate.save_data_in_filesystem(df=testing_data,filename=test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ed16fe2-48e7-4afc-8cb5-562e90079303"
   },
   "source": [
    "## Check if files Exists in COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25d715cf-f7b2-4106-9259-0f1a5762710a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Replace with Db2/fileystem\n",
    "files_copied_in_cos = check_for_file_in_filesystem(train_data_path) and check_for_file_in_filesystem(test_data_path)\n",
    "files_copied_in_cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b42568a9-36f8-407e-be7a-f8bbbf93444a"
   },
   "source": [
    "## Register a Boolean Variable in WS Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f210efd-7358-41a4-9ab5-37a856f3ab47",
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_params = {}\n",
    "validation_params['anomaly_status'] = anomaly_status\n",
    "validation_params['files_copied_in_cos'] = files_copied_in_cos\n",
    "validation_params['train_data_filename'] = train_data_filename\n",
    "validation_params['test_data_filename'] = test_data_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf0ff093-af79-4bb5-bd22-d2d5315926e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipelines_client = WSPipelines.from_token(token=TOKEN)\n",
    "pipelines_client = WSPipelines.from_token(TOKEN)\n",
    "pipelines_client.store_results(validation_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
